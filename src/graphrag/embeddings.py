"""
åµŒå…¥å¤„ç†æ¨¡å—
å‘é‡åŒ–å¼•æ“ - åŒ»å­¦æ–‡æ¡£çš„å‘é‡åŒ–å¤„ç†
"""
import os
import json
import logging
import re
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from sentence_transformers import SentenceTransformer
import pickle
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    """æ–‡æ¡£åˆ†å—æ•°æ®ç±»"""
    chunk_id: str
    text: str
    source_file: str
    metadata: Dict[str, Any]
    vector: Optional[np.ndarray] = None

class EmbeddingEngine:
    """å‘é‡åŒ–å¼•æ“ç±»"""
    
    def __init__(self, 
                 model_name: str = "all-mpnet-base-v2",#"NeuML/pubmedbert-base-embeddings" åŒ»å­¦ä¸“ç”¨æ¨¡å‹,ä½†ç”µè„‘ç¼“å­˜ä¸ä¸‹æ¥ï¼Œå¯è€ƒè™‘åç»­æ”¹è¿›
                 chunk_size: int = 512,
                 overlap_size: int = 50):
        """
        åˆå§‹åŒ–å‘é‡åŒ–å¼•æ“
        
        Args:
            model_name: å‘é‡åŒ–æ¨¡å‹åç§°ï¼ˆä½¿ç”¨åŒ»å­¦ä¸“ç”¨æ¨¡å‹ï¼‰
            chunk_size: åˆ†å—å¤§å°
            overlap_size: é‡å å¤§å°
        """
        self.model_name = model_name
        self.chunk_size = chunk_size
        self.overlap_size = overlap_size
        self.model = None
        self.chunks: List[DocumentChunk] = []
        self.vectors: Optional[np.ndarray] = None
        
        # è®¾ç½®å­˜å‚¨è·¯å¾„
        self.vector_store_path = Path("models/vectors")
        self.vector_store_path.mkdir(parents=True, exist_ok=True)
        
        # åŒ»å­¦æœ¯è¯­ä¿æŠ¤æ¨¡å¼ - å…³é”®åŒ»å­¦å®ä½“ä¸è¢«åˆ†å‰²
        self.medical_entities = [
            r'å¦Šå¨ æœŸç³–å°¿ç—…[^ã€‚]*?ã€‚',  # GDMç›¸å…³
            r'gestational diabetes mellitus[^ã€‚]*?ã€‚',
            r'è¡€ç³–[ç›‘æµ‹|æ§åˆ¶|ç®¡ç†][^ã€‚]*?ã€‚',  # è¡€ç³–ç›¸å…³
            r'èƒ°å²›ç´ [æ²»ç–—|æ³¨å°„|ä½¿ç”¨][^ã€‚]*?ã€‚',  # èƒ°å²›ç´ ç›¸å…³
            r'å­•æœŸ[è¥å…»|ç®¡ç†|ä¿å¥][^ã€‚]*?ã€‚',  # å­•æœŸç›¸å…³
            r'å›´äº§æœŸ[å¹¶å‘ç—‡|ç®¡ç†][^ã€‚]*?ã€‚',  # å›´äº§æœŸç›¸å…³
        ]
        
        # è®¾ç½®æ¨¡å‹ç¼“å­˜ç­–ç•¥
        self._setup_cache_strategy()
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¸¦ç¼“å­˜æ£€æŸ¥ï¼‰
        self._load_model_with_cache_check()
    
    def _setup_cache_strategy(self):
        """è®¾ç½®æ¨¡å‹ç¼“å­˜ç­–ç•¥"""
        # è®¾ç½®Hugging Faceç¼“å­˜ç›®å½•ï¼ˆç¡®ä¿æŒä¹…åŒ–ï¼‰
        cache_dir = os.path.expanduser("~/.cache/huggingface")
        os.makedirs(cache_dir, exist_ok=True)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['TRANSFORMERS_CACHE'] = cache_dir
        os.environ['HF_HOME'] = cache_dir
        
        logger.info(f"ğŸ“ æ¨¡å‹ç¼“å­˜ç›®å½•: {cache_dir}")
    
    def _check_model_cached(self, model_name):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»ç¼“å­˜åˆ°æœ¬åœ° - ä½¿ç”¨ local_files_only"""
        try:
            start_time = time.time()
        
            # ä½¿ç”¨ local_files_only=True å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
            model = SentenceTransformer(model_name, local_files_only=True)
            load_time = time.time() - start_time
        
            logger.info(f"âœ… æ¨¡å‹å·²ç¼“å­˜ï¼Œç¦»çº¿åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
            return model
        
        except Exception as e:
            logger.info(f"æ¨¡å‹æœªå®Œæ•´ç¼“å­˜ï¼Œéœ€è¦ä¸‹è½½: {str(e)[:100]}...")
            return None
    
    def _download_model_once(self, model_name):
        """ä¸€æ¬¡æ€§ä¸‹è½½æ¨¡å‹åˆ°ç¼“å­˜ - ä¸ä½¿ç”¨ local_files_only"""
        logger.info(f"æ­£åœ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜: {model_name}")
        logger.info("âš ï¸  é¦–æ¬¡ä¸‹è½½éœ€è¦ç½‘ç»œè¿æ¥ï¼Œä¸‹è½½åå¯ç¦»çº¿ä½¿ç”¨")
        logger.info("ğŸ’¡ å¦‚æœä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä»£ç†è®¾ç½®")
    
        try:
            # ä¸‹è½½æ—¶ä¸ä½¿ç”¨ local_files_onlyï¼Œå…è®¸ç½‘ç»œè®¿é—®
            model = SentenceTransformer(model_name)
            logger.info("âœ… æ¨¡å‹ä¸‹è½½å¹¶ç¼“å­˜æˆåŠŸï¼")
            logger.info("ğŸ‰ ä¸‹æ¬¡å¯åŠ¨å°†ä»æœ¬åœ°ç¼“å­˜åŠ è½½ï¼Œæ— éœ€ç½‘ç»œè¿æ¥")
            return model
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            logger.error("ğŸ’¡ è§£å†³å»ºè®®ï¼š")
            logger.error("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            logger.error("   2. å¦‚æœåœ¨å›½å†…ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ä»£ç†")
            logger.error("   3. æˆ–è¿è¡Œ python download_models.py é¢„ä¸‹è½½")
            raise
    
    def _load_model_with_cache_check(self):
        """å¸¦ç¼“å­˜æ£€æŸ¥çš„æ¨¡å‹åŠ è½½ - ä¼˜å…ˆä½¿ç”¨ç¨³å®šæ¨¡å‹"""
        
        # æŒ‰ç¨³å®šæ€§æ’åºçš„æ¨¡å‹åˆ—è¡¨
        model_candidates = [
            "all-mpnet-base-v2",          # 420MB - è´¨é‡å¥½ï¼ˆå½“å‰ä½¿ç”¨çš„ï¼‰
            "all-MiniLM-L6-v2",           # 22MB - æœ€ç¨³å®š
            "paraphrase-MiniLM-L6-v2",    # 22MB - å¤‡é€‰
            "NeuML/pubmedbert-base-embeddings"  # 438MB - åŒ»å­¦ä¸“ç”¨ï¼ˆæœ€åå°è¯•ï¼‰
        ]
        
        for i, model_name in enumerate(model_candidates, 1):
            try:
                logger.info(f"å°è¯•æ¨¡å‹ {i}/{len(model_candidates)}: {model_name}")
                
                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
                cached_model = self._check_model_cached(model_name)
                
                if cached_model:
                    # ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹
                    self.model = cached_model
                    self.model_name = model_name
                    logger.info(f"âœ… ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹: {model_name}")
                else:
                    # éœ€è¦ä¸‹è½½
                    self.model = self._download_model_once(model_name)
                    self.model_name = model_name
                
                # éªŒè¯æ¨¡å‹åŠŸèƒ½
                logger.info("éªŒè¯æ¨¡å‹åŠŸèƒ½...")
                test_vector = self.model.encode(["æµ‹è¯•æ–‡æœ¬", "å¦Šå¨ æœŸç³–å°¿ç—…"])
                
                logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
                logger.info(f"   å‘é‡ç»´åº¦: {test_vector.shape}")
                logger.info(f"   è®¾å¤‡: {self.model.device}")
                
                return  # æˆåŠŸåé€€å‡º
                
            except Exception as e:
                logger.warning(f"âŒ æ¨¡å‹ {model_name} å¤±è´¥: {str(e)[:100]}...")
                
                # å¦‚æœæ˜¯æ–‡ä»¶æŸåï¼Œå°è¯•æ¸…ç†ç¼“å­˜
                if "No such file or directory" in str(e) or "FileNotFoundError" in str(e):
                    logger.info(f"æ£€æµ‹åˆ°ç¼“å­˜æ–‡ä»¶æŸåï¼Œæ¸…ç† {model_name} ç¼“å­˜...")
                    try:
                        import shutil
                        cache_path = os.path.expanduser(
                            f"~/.cache/huggingface/hub/models--{model_name.replace('/', '--')}"
                        )
                        if os.path.exists(cache_path):
                            shutil.rmtree(cache_path)
                            logger.info("ç¼“å­˜å·²æ¸…ç†")
                    except Exception as cleanup_e:
                        logger.warning(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {cleanup_e}")
                
                continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
        
        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†
        logger.error("âŒ æ‰€æœ‰é¢„è®¾æ¨¡å‹éƒ½åŠ è½½å¤±è´¥")
        logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
        logger.error("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        logger.error("   2. è¿è¡Œ python download_models.py é¢„ä¸‹è½½æ¨¡å‹")
        logger.error("   3. æˆ–åœ¨æœ‰ç½‘ç»œç¯å¢ƒä¸‹å…ˆè¿è¡Œä¸€æ¬¡")
        raise Exception("æ— æ³•åŠ è½½ä»»ä½•å‘é‡åŒ–æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é¢„ä¸‹è½½æ¨¡å‹")
    
    def _smart_text_chunking(self, text: str, source_file: str = "") -> List[str]:
        """
        æ™ºèƒ½åŒ»å­¦æ–‡æœ¬åˆ†å— - ä¿æŒåŒ»å­¦æœ¯è¯­å’Œæ¦‚å¿µå®Œæ•´æ€§
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            source_file: æºæ–‡ä»¶å
            
        Returns:
            åˆ†å—åçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not text.strip():
            return []
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\n+', '\n', text)  # åˆå¹¶å¤šä¸ªæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text)   # åˆå¹¶å¤šä¸ªç©ºæ ¼
        
        # æŒ‰å¥å­åˆ†å‰²ï¼ˆæ”¯æŒä¸­è‹±æ–‡æ ‡ç‚¹ï¼‰
        sentence_pattern = r'[ã€‚.!ï¼ï¼Ÿ?ï¼›;]\s*'
        sentences = re.split(sentence_pattern, text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return [text] if len(text) <= self.chunk_size else self._force_split(text)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # ä¼°ç®—åŠ å…¥è¿™ä¸ªå¥å­åçš„é•¿åº¦
            temp_chunk = current_chunk + sentence + "ã€‚"
            
            if len(temp_chunk) <= self.chunk_size:
                current_chunk = temp_chunk
            else:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # å¦‚æœå•ä¸ªå¥å­å¤ªé•¿ï¼Œå¼ºåˆ¶åˆ†å‰²
                if len(sentence) > self.chunk_size:
                    forced_chunks = self._force_split(sentence)
                    chunks.extend(forced_chunks)
                    current_chunk = ""
                else:
                    current_chunk = sentence + "ã€‚"
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # æ·»åŠ é‡å å¤„ç†
        if len(chunks) > 1 and self.overlap_size > 0:
            chunks = self._add_overlap(chunks)
        
        return [chunk for chunk in chunks if chunk.strip()]
    
    def _force_split(self, text: str) -> List[str]:
        """å¼ºåˆ¶åˆ†å‰²è¿‡é•¿æ–‡æœ¬"""
        chunks = []
        words = text.split()
        
        current_chunk_words = []
        current_length = 0
        
        for word in words:
            word_length = len(word)
            if current_length + word_length <= self.chunk_size:
                current_chunk_words.append(word)
                current_length += word_length + 1  # +1 for space
            else:
                if current_chunk_words:
                    chunks.append(' '.join(current_chunk_words))
                current_chunk_words = [word]
                current_length = word_length
        
        if current_chunk_words:
            chunks.append(' '.join(current_chunk_words))
        
        return chunks
    
    def _add_overlap(self, chunks: List[str]) -> List[str]:
        """ä¸ºåˆ†å—æ·»åŠ é‡å å†…å®¹"""
        if len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = [chunks[0]]  # ç¬¬ä¸€ä¸ªå—ä¸éœ€è¦é‡å 
        
        for i in range(1, len(chunks)):
            prev_chunk = chunks[i-1]
            current_chunk = chunks[i]
            
            # ä»å‰ä¸€ä¸ªå—æœ«å°¾æå–é‡å å†…å®¹
            prev_words = prev_chunk.split()
            if len(prev_words) > self.overlap_size:
                overlap_words = prev_words[-self.overlap_size:]
                overlap_text = ' '.join(overlap_words)
                
                # æ·»åŠ é‡å å†…å®¹åˆ°å½“å‰å—
                overlapped_chunk = f"{overlap_text} {current_chunk}"
                overlapped_chunks.append(overlapped_chunk)
            else:
                overlapped_chunks.append(current_chunk)
        
        return overlapped_chunks
    
    def process_documents(self, data_dir: str = "data/processed") -> int:
        """
        å¤„ç†æ‰€æœ‰æ–‡æ¡£å¹¶ç”Ÿæˆåˆ†å—
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            
        Returns:
            å¤„ç†çš„æ–‡æ¡£æ•°é‡
        """
        logger.info("å¼€å§‹å¤„ç†æ–‡æ¡£...")
        
        processed_count = 0
        chunk_id = 0
        
        # å¤„ç†å„ç±»åŒ»å­¦æ•°æ®
        data_types = ["guidelines", "pubmed", "textbooks", "faq"]
        
        for data_type in data_types:
            type_dir = Path(data_dir) / data_type
            if not type_dir.exists():
                logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {type_dir}")
                continue
            
            logger.info(f"å¤„ç† {data_type} æ•°æ®...")
            
            # éå†æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
            for text_file in type_dir.glob("*.txt"):
                try:
                    with open(text_file, 'r', encoding='utf-8') as f:
                        text_content = f.read()
                    
                    if not text_content.strip():
                        logger.warning(f"ç©ºæ–‡ä»¶: {text_file}")
                        continue
                    
                    # æ™ºèƒ½åˆ†å—å¤„ç†
                    chunks = self._smart_text_chunking(text_content, str(text_file))
                    
                    for i, chunk_text in enumerate(chunks):
                        chunk = DocumentChunk(
                            chunk_id=f"{data_type}_{chunk_id}",
                            text=chunk_text,
                            source_file=str(text_file),
                            metadata={
                                "data_type": data_type,
                                "source_name": text_file.stem,
                                "chunk_index": i,
                                "chunk_length": len(chunk_text),
                                "total_chunks_in_doc": len(chunks)
                            }
                        )
                        self.chunks.append(chunk)
                        chunk_id += 1
                    
                    processed_count += 1
                    logger.info(f"âœ… {text_file.name}: {len(chunks)} ä¸ªåˆ†å—")
                
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {text_file}: {e}")
        
        logger.info(f"ğŸ“„ æ–‡æ¡£å¤„ç†å®Œæˆï¼{processed_count} ä¸ªæ–‡ä»¶ï¼Œ{len(self.chunks)} ä¸ªåˆ†å—")
        return processed_count
    
    def generate_embeddings(self, batch_size: int = 32) -> bool:
        """
        æ‰¹é‡ç”Ÿæˆå‘é‡åµŒå…¥
        
        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ˜¯å¦æˆåŠŸç”Ÿæˆå‘é‡
        """
        if not self.chunks:
            logger.error("æ²¡æœ‰å¾…å¤„ç†çš„æ–‡æ¡£åˆ†å—")
            return False
        
        if not self.model:
            logger.error("å‘é‡åŒ–æ¨¡å‹æœªåŠ è½½")
            return False
        
        logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆ {len(self.chunks)} ä¸ªåˆ†å—çš„å‘é‡...")
        
        try:
            # æå–æ‰€æœ‰æ–‡æœ¬
            texts = [chunk.text for chunk in self.chunks]
            
            # æ‰¹é‡ç”Ÿæˆå‘é‡ï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
            vectors = self.model.encode(
                texts, 
                show_progress_bar=True, 
                batch_size=batch_size,
                convert_to_numpy=True
            )
            
            # å­˜å‚¨å‘é‡åˆ°æ¯ä¸ªåˆ†å—
            for i, chunk in enumerate(self.chunks):
                chunk.vector = vectors[i]
            
            # ä¿å­˜å‘é‡çŸ©é˜µ
            self.vectors = np.array(vectors)
            
            logger.info(f"âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼ç»´åº¦: {self.vectors.shape}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return False
    
    def save_vectors(self, filename: str = "medical_document_vectors.pkl") -> str:
        """
        ä¿å­˜å‘é‡æ•°æ®
        
        Args:
            filename: ä¿å­˜æ–‡ä»¶å
            
        Returns:
            ä¿å­˜æ–‡ä»¶è·¯å¾„
        """
        if not self.chunks or not self.vectors is not None:
            logger.error("æ²¡æœ‰å‘é‡æ•°æ®å¯ä¿å­˜")
            return ""
        
        save_path = self.vector_store_path / filename
        
        try:
            # å‡†å¤‡ä¿å­˜æ•°æ®
            save_data = {
                "metadata": {
                    "model_name": self.model_name,
                    "chunk_size": self.chunk_size,
                    "overlap_size": self.overlap_size,
                    "total_chunks": len(self.chunks),
                    "vector_dimension": self.vectors.shape[1] if self.vectors is not None else 0,
                    "version": "2.0"
                },
                "chunks": [
                    {
                        "chunk_id": chunk.chunk_id,
                        "text": chunk.text,
                        "source_file": chunk.source_file,
                        "metadata": chunk.metadata,
                        "vector": chunk.vector.tolist() if chunk.vector is not None else None
                    }
                    for chunk in self.chunks
                ]
            }
            
            # ä¿å­˜ä¸ºpickleæ ¼å¼
            with open(save_path, 'wb') as f:
                pickle.dump(save_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            # ä¿å­˜æ‘˜è¦ä¿¡æ¯ä¸ºJSON
            summary_path = save_path.with_suffix('.json')
            summary_data = {
                "summary": save_data["metadata"],
                "data_type_distribution": self._get_type_distribution(),
                "sample_chunks": [
                    {
                        "chunk_id": chunk.chunk_id,
                        "text_preview": chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text,
                        "source": chunk.metadata.get("source_name", ""),
                        "data_type": chunk.metadata.get("data_type", "")
                    }
                    for chunk in self.chunks[:5]
                ]
            }
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… å‘é‡æ•°æ®å·²ä¿å­˜: {save_path}")
            logger.info(f"ğŸ“‹ æ‘˜è¦ä¿¡æ¯å·²ä¿å­˜: {summary_path}")
            
            return str(save_path)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å‘é‡å¤±è´¥: {e}")
            return ""
    
    def load_vectors(self, filename: str = "medical_document_vectors.pkl") -> bool:
        """
        åŠ è½½å‘é‡æ•°æ®
        
        Args:
            filename: å‘é‡æ–‡ä»¶å
            
        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        load_path = self.vector_store_path / filename
        
        if not load_path.exists():
            logger.error(f"å‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
            return False
        
        try:
            with open(load_path, 'rb') as f:
                data = pickle.load(f)
            
            # åŠ è½½å…ƒæ•°æ®
            if "metadata" in data:
                metadata = data["metadata"]
                logger.info(f"ğŸ“‹ åŠ è½½å‘é‡æ•°æ®ç‰ˆæœ¬: {metadata.get('version', 'unknown')}")
                logger.info(f"ğŸ“Š æ¨¡å‹: {metadata.get('model_name', 'unknown')}")
                logger.info(f"ğŸ“ å‘é‡ç»´åº¦: {metadata.get('vector_dimension', 'unknown')}")
            
            # é‡æ„åˆ†å—æ•°æ®
            self.chunks = []
            for chunk_data in data["chunks"]:
                chunk = DocumentChunk(
                    chunk_id=chunk_data["chunk_id"],
                    text=chunk_data["text"],
                    source_file=chunk_data["source_file"],
                    metadata=chunk_data["metadata"],
                    vector=np.array(chunk_data["vector"]) if chunk_data["vector"] else None
                )
                self.chunks.append(chunk)
            
            # é‡æ„å‘é‡çŸ©é˜µ
            if self.chunks and self.chunks[0].vector is not None:
                self.vectors = np.array([chunk.vector for chunk in self.chunks])
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.chunks)} ä¸ªå‘é‡åˆ†å—")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å‘é‡å¤±è´¥: {e}")
            return False
    
    def similarity_search(self, 
                         query: str, 
                         top_k: int = 5, 
                         filter_type: Optional[str] = None,
                         min_score: float = 0.0) -> List[Tuple[DocumentChunk, float]]:
        """
        è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æœ€ç›¸ä¼¼çš„Kä¸ªç»“æœ
            filter_type: è¿‡æ»¤æ–‡æ¡£ç±»å‹ ('guidelines', 'pubmed', 'textbooks', 'faq')
            min_score: æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°é˜ˆå€¼
            
        Returns:
            ç›¸ä¼¼åº¦æœ€é«˜çš„åˆ†å—åˆ—è¡¨ï¼Œæ ¼å¼ä¸º (chunk, similarity_score)
        """
        if not self.chunks or self.vectors is None:
            logger.warning("æ²¡æœ‰å¯æœç´¢çš„å‘é‡æ•°æ®")
            return []
        
        if not self.model:
            logger.error("å‘é‡åŒ–æ¨¡å‹æœªåŠ è½½")
            return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.model.encode([query], convert_to_numpy=True)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = np.dot(self.vectors, query_vector.T).flatten()
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            valid_results = []
            for i, (chunk, score) in enumerate(zip(self.chunks, similarities)):
                # ç±»å‹è¿‡æ»¤
                if filter_type and chunk.metadata.get('data_type') != filter_type:
                    continue
                
                # åˆ†æ•°è¿‡æ»¤
                if score < min_score:
                    continue
                
                valid_results.append((i, chunk, float(score)))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            valid_results.sort(key=lambda x: x[2], reverse=True)
            
            # è¿”å›top-kç»“æœ
            top_results = valid_results[:top_k]
            
            return [(chunk, score) for _, chunk, score in top_results]
            
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            return []
    
    def _get_type_distribution(self) -> Dict[str, int]:
        """è·å–æ–‡æ¡£ç±»å‹åˆ†å¸ƒ"""
        type_counts = {}
        for chunk in self.chunks:
            doc_type = chunk.metadata.get('data_type', 'unknown')
            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
        return type_counts
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.chunks:
            return {"total_chunks": 0, "status": "empty"}
        
        lengths = [len(chunk.text) for chunk in self.chunks]
        
        stats = {
            "basic_info": {
                "total_chunks": len(self.chunks),
                "model_name": self.model_name,
                "chunk_size": self.chunk_size,
                "overlap_size": self.overlap_size,
                "vector_dimension": self.vectors.shape[1] if self.vectors is not None else 0
            },
            "type_distribution": self._get_type_distribution(),
            "text_statistics": {
                "min_length": min(lengths),
                "max_length": max(lengths),
                "avg_length": sum(lengths) / len(lengths),
                "total_characters": sum(lengths)
            },
            "status": "ready" if self.vectors is not None else "vectors_not_generated"
        }
        
        return stats
    
    def clear_data(self):
        """æ¸…ç©ºæ‰€æœ‰æ•°æ®"""
        self.chunks.clear()
        self.vectors = None
        logger.info("ğŸ§¹ å‘é‡æ•°æ®å·²æ¸…ç©º")

# ä¾¿æ·å‡½æ•°
def create_embedding_engine(**kwargs) -> EmbeddingEngine:
    """åˆ›å»ºå‘é‡åŒ–å¼•æ“å®ä¾‹"""
    return EmbeddingEngine(**kwargs)

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨åŒ»å­¦æ–‡æ¡£å‘é‡åŒ–å¼•æ“...")
    
    # åˆ›å»ºå¼•æ“
    engine = EmbeddingEngine()
    
    # å¤„ç†æ–‡æ¡£
    count = engine.process_documents()
    print(f"ğŸ“„ å¤„ç†äº† {count} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    
    if count > 0:
        # ç”Ÿæˆå‘é‡
        if engine.generate_embeddings():
            print(f"ğŸ¯ ç”Ÿæˆäº† {len(engine.chunks)} ä¸ªåˆ†å—çš„å‘é‡")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = engine.get_statistics()
            print(f"\nğŸ“Š å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æ€»åˆ†å—æ•°: {stats['basic_info']['total_chunks']}")
            print(f"   å‘é‡ç»´åº¦: {stats['basic_info']['vector_dimension']}")
            print(f"   å¹³å‡é•¿åº¦: {stats['text_statistics']['avg_length']:.0f} å­—ç¬¦")
            print(f"   ç±»å‹åˆ†å¸ƒ: {stats['type_distribution']}")
            
            # ä¿å­˜å‘é‡
            save_path = engine.save_vectors()
            if save_path:
                print(f"ğŸ’¾ å‘é‡å·²ä¿å­˜åˆ°: {save_path}")
                
                # æµ‹è¯•æœç´¢
                print(f"\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½:")
                test_queries = [
                    "å¦Šå¨ æœŸç³–å°¿ç—…çš„ç—‡çŠ¶",
                    "è¡€ç³–ç›‘æµ‹æ–¹æ³•",
                    "å­•æœŸè¥å…»ç®¡ç†"
                ]
                
                for query in test_queries:
                    results = engine.similarity_search(query, top_k=2)
                    print(f"\næŸ¥è¯¢: '{query}'")
                    for i, (chunk, score) in enumerate(results, 1):
                        print(f"  {i}. ç›¸ä¼¼åº¦: {score:.3f} | ç±»å‹: {chunk.metadata['data_type']}")
                        print(f"     å†…å®¹: {chunk.text[:80]}...")
        
    print("\nâœ… å‘é‡åŒ–å¼•æ“æµ‹è¯•å®Œæˆ!")
