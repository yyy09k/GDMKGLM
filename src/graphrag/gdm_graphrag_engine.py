"""
GDM GraphRAGä¸»å¼•æ“
æ•´åˆæ‰€æœ‰RAGç»„ä»¶ï¼Œå®ç°å®Œæ•´çš„æŸ¥è¯¢å¤„ç†æµç¨‹
"""

import os
import re
import sys
import logging
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../.."))
sys.path.insert(0, project_root)

# å¯¼å…¥å·²å®Œæˆçš„æ¨¡å—
from src.graphrag.hybrid_retriever import HybridRetriever, HybridSearchResult
from src.graphrag.prompt_templates import create_gdm_prompt_interface, GraphRAGPromptInterface
from src.utils.deepseek_client import DeepSeekClient

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GDMRAGResponse:
    """GDM RAGç³»ç»Ÿå“åº”ç»“æœ"""
    question: str
    answer: str
    sources: List[str]
    context_used: str
    response_time: float
    confidence_score: float
    retrieval_stats: Dict[str, Any]
    query_analysis: Dict[str, Any]

class GDMGraphRAGEngine:
    """GDM GraphRAGä¸»å¼•æ“"""
    
    def __init__(self, 
                 deepseek_api_key: str = "sk-f73a7b96600a4eeebe34cbe357902568",
                 enable_cache: bool = True,
                 max_context_length: int = 3500):
        """
        åˆå§‹åŒ–GDM GraphRAGå¼•æ“
        
        Args:
            deepseek_api_key: DeepSeek APIå¯†é’¥
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        """
        self.enable_cache = enable_cache
        self.max_context_length = max_context_length
        self.cache: Dict[str, GDMRAGResponse] = {}
        
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–GDM GraphRAGå¼•æ“...")
            
            # 1. åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨ - ä½¿ç”¨HybridRetriever
            logger.info("åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨...")
            self.hybrid_retriever = HybridRetriever(
                embedding_model="all-mpnet-base-v2",
                semantic_weight=0.6,
                graph_weight=0.4,
                max_semantic_results=5,
                max_graph_results=3
            )
            logger.info("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # 2. åˆå§‹åŒ–æç¤ºè¯æ¥å£ - ä½¿ç”¨GraphRAGPromptInterface
            logger.info("åˆå§‹åŒ–æç¤ºè¯æ¥å£...")
            prompt_config = {
                'max_tokens': max_context_length,
                'enable_safety_enhancement': True,
                'enable_optimization': True
            }
            self.prompt_interface = create_gdm_prompt_interface(prompt_config)
            logger.info("âœ… æç¤ºè¯æ¥å£åˆå§‹åŒ–å®Œæˆ")
            
            # 3. åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯ - ä½¿ç”¨DeepSeekClient
            logger.info("åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯...")
            self.deepseek_client = DeepSeekClient(api_key=deepseek_api_key)
            logger.info("âœ… DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
            
            logger.info("ğŸ‰ GDM GraphRAGå¼•æ“åˆå§‹åŒ–æˆåŠŸï¼")
            
        except Exception as e:
            logger.error(f"âŒ GraphRAGå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _generate_cache_key(self, question: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        return hashlib.md5(question.encode('utf-8')).hexdigest()
    
    def _simple_query_classification(self, query: str) -> str:
        """
        ç®€å•æŸ¥è¯¢åˆ†ç±» - é€‚é…HybridRetrieverçš„classify_query_typeæ–¹æ³•
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            æŸ¥è¯¢ç±»å‹
        """
        return self.hybrid_retriever.classify_query_type(query)
    
    def _extract_sources(self, retrieval_result: HybridSearchResult) -> List[str]:
        """
        æå–ä¿¡æ¯æ¥æº
        
        Args:
            retrieval_result: æ··åˆæ£€ç´¢ç»“æœ
            
        Returns:
            ä¿¡æ¯æ¥æºåˆ—è¡¨
        """
        sources = []
        
        # ä»è¯­ä¹‰æ£€ç´¢ç»“æœä¸­æå–æ¥æº
        for chunk, score in retrieval_result.semantic_results:
            # ä½¿ç”¨chunkçš„source_fileå±æ€§
            if hasattr(chunk, 'source_file') and chunk.source_file:
                source_name = os.path.basename(chunk.source_file).replace('.txt', '')
                source_info = f"æ–‡æ¡£:{source_name}"
                if source_info not in sources:
                    sources.append(source_info)
            
            # å¦‚æœæœ‰metadataï¼Œæå–data_type
            if hasattr(chunk, 'metadata') and chunk.metadata:
                data_type = chunk.metadata.get('data_type', 'unknown')
                type_info = f"ç±»å‹:{data_type}"
                if type_info not in sources:
                    sources.append(type_info)
        
        # ä»å›¾è°±æ£€ç´¢ç»“æœä¸­æå–æ¥æº - æ·»åŠ å®‰å…¨æ£€æŸ¥
        if hasattr(retrieval_result, 'graph_results') and retrieval_result.graph_results:
            graph_entities = []
            for graph_result in retrieval_result.graph_results:
                if hasattr(graph_result, 'entities') and graph_result.entities:
                    entity_names = [entity.name for entity in graph_result.entities[:2]]
                    graph_entities.extend(entity_names)
            
            if graph_entities:
                sources.append(f"çŸ¥è¯†å›¾è°±:{','.join(graph_entities[:3])}")
        
        return sources[:5]  # é™åˆ¶æ¥æºæ•°é‡
    
    def _calculate_confidence_score(self, retrieval_result: HybridSearchResult) -> float:
        """
        è®¡ç®—ç½®ä¿¡åº¦ - åŸºäºHybridSearchResultç»“æ„
        
        Args:
            retrieval_result: æ··åˆæ£€ç´¢ç»“æœ
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0-1)
        """
        # ä½¿ç”¨HybridSearchResultçš„final_scoreä½œä¸ºåŸºç¡€ç½®ä¿¡åº¦
        base_confidence = retrieval_result.final_score
        
        # æ ¹æ®æ£€ç´¢ç»“æœè´¨é‡è°ƒæ•´
        quality_bonus = 0.0
        
        # è¯­ä¹‰æ£€ç´¢è´¨é‡åŠ æˆ
        if retrieval_result.semantic_results:
            avg_semantic_score = sum(score for _, score in retrieval_result.semantic_results) / len(retrieval_result.semantic_results)
            quality_bonus += avg_semantic_score * 0.1
        
        # å›¾è°±æ£€ç´¢è´¨é‡åŠ æˆ
        if retrieval_result.graph_results:
            avg_graph_score = sum(gr.relevance_score for gr in retrieval_result.graph_results) / len(retrieval_result.graph_results)
            quality_bonus += avg_graph_score * 0.1
        
        # ä¸Šä¸‹æ–‡é•¿åº¦åŠ æˆ
        if len(retrieval_result.combined_context) > 500:
            quality_bonus += 0.05
        
        final_confidence = min(base_confidence + quality_bonus, 1.0)
        return max(final_confidence, 0.1)  # ä¿è¯æœ€ä½ç½®ä¿¡åº¦
    
    def _post_process_answer(self, answer: str, question: str) -> str:
        """
        åå¤„ç†å›ç­”å†…å®¹ - ä¼˜åŒ–ç”¨æˆ·é˜…è¯»ä½“éªŒ
        """
        if not answer or not answer.strip():
            return "æŠ±æ­‰ï¼Œç³»ç»Ÿæ— æ³•ç”Ÿæˆåˆé€‚çš„å›ç­”ï¼Œè¯·å°è¯•æ¢ä¸ªé—®é¢˜æˆ–å’¨è¯¢ä¸“ä¸šåŒ»å¸ˆã€‚"
    
        # æ¸…ç†å›ç­”
        answer = answer.strip()
    
        # 1. å¼ºåŒ–ç§»é™¤æ‰€æœ‰Markdownæ ¼å¼æ ‡è®°
        # ç§»é™¤æ ‡é¢˜æ ‡è®° ### text -> text, ## text -> text, # text -> text
        answer = re.sub(r'^#{1,6}\s*(.*)', r'\1', answer, flags=re.MULTILINE)
    
        # ç§»é™¤ç²—ä½“æ ‡è®° **text** -> text
        answer = re.sub(r'\*\*(.*?)\*\*', r'\1', answer)
    
        # ç§»é™¤æ–œä½“æ ‡è®° *text* -> text  
        answer = re.sub(r'\*(.*?)\*', r'\1', answer)
    
        # ç§»é™¤ä»£ç å—æ ‡è®°
        answer = re.sub(r'```.*?```', '', answer, flags=re.DOTALL)
        answer = re.sub(r'`(.*?)`', r'\1', answer)
    
        # ç§»é™¤é“¾æ¥æ ¼å¼ [text](url) -> text
        answer = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', answer)
    
        # ç§»é™¤åˆ—è¡¨æ ‡è®°ä½†ä¿ç•™å†…å®¹ - å¤„ç† Markdown åˆ—è¡¨
        answer = re.sub(r'^\s*[-\*\+]\s+', '', answer, flags=re.MULTILINE)
    
        # 2. é¢„å¤„ç† - ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        answer = answer.replace('ï¼š', ':').replace('ï¼Œ', ',')
    
        # 3. æ™ºèƒ½åˆ†æ®µå¤„ç†
        answer = self._smart_paragraph_split(answer)
    
        # 4. ç§»é™¤å¯èƒ½çš„æ¨¡æ¿æ®‹ç•™
        unwanted_prefixes = [
            'æ ¹æ®æä¾›çš„ä¿¡æ¯', 'åŸºäºä»¥ä¸Šå†…å®¹', 'ä»ä¸Šè¿°èµ„æ–™å¯ä»¥çœ‹å‡º',
            'æ ¹æ®ç›¸å…³èµ„æ–™', 'åŸºäºä¸“ä¸šçŸ¥è¯†', 'ä»åŒ»å­¦è§’åº¦æ¥çœ‹',
            'æ ¹æ®åŒ»å­¦çŸ¥è¯†', 'ä»ä¸´åºŠè§’åº¦'
        ]
        for prefix in unwanted_prefixes:
            if answer.startswith(prefix):
                answer = answer[len(prefix):].lstrip('ï¼Œ,ï¼š: ')
    
        # 5. ç¡®ä¿å›ç­”å®Œæ•´æ€§
        if not answer.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            answer += 'ã€‚'
    
        # 6. æœ€ç»ˆæ ¼å¼åŒ–å¤„ç†
        answer = self._final_format_processing(answer)
    
        return answer

    def _smart_paragraph_split(self, text: str) -> str:
        """
        æ™ºèƒ½åˆ†æ®µå¤„ç† - å¼ºåŒ–æ•°å­—åˆ—è¡¨åˆ†æ®µ
        """
        if len(text) < 80:
            return text

        # é¦–å…ˆæŒ‰ç°æœ‰æ®µè½åˆ†å‰²
        existing_paragraphs = re.split(r'\n\s*\n|\n', text)
        existing_paragraphs = [p.strip() for p in existing_paragraphs if p.strip()]
    
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬é‡æ–°å¤„ç†
        full_text = ' '.join(existing_paragraphs)
    
        # ç‰¹æ®Šæ ‡è®°å¼ºåˆ¶åˆ†æ®µ
        section_markers = [
            r'ğŸ“š\s*è¯¦ç»†è¯´æ˜',
            r'ğŸ“š\s*',
            r'â—\s*ç›¸å…³æé†’', 
            r'â—\s*',
            r'ğŸ¥\s*è¡ŒåŠ¨å»ºè®®',
            r'ğŸ¥\s*',
            r'ğŸ’¡\s*æ¸©é¦¨æç¤º',
            r'ğŸ’¡\s*',
            r'âš ï¸\s*æ³¨æ„äº‹é¡¹',
            r'âš ï¸\s*',
            r'ğŸ”\s*è¯Šæ–­æ ‡å‡†',
            r'ğŸ”\s*',
            r'ğŸ\s*é¥®é£Ÿå»ºè®®',
            r'ğŸ\s*',
            r'ğŸ’Š\s*æ²»ç–—æ–¹æ¡ˆ',
            r'ğŸ’Š\s*',
            r'æ ¸å¿ƒå›ç­”[ï¼ˆ(]åŸºäºçŸ¥è¯†å›¾è°±[ï¼‰)]',
            r'æ ¹æ®åŒ»å­¦çŸ¥è¯†å›¾è°±',
            r'ä¸»è¦ç—‡çŠ¶åŒ…æ‹¬[:ï¼š]',
            r'æ²»ç–—æ–¹æ¡ˆå¦‚ä¸‹[:ï¼š]',
            r'é¢„é˜²æªæ–½åŒ…æ‹¬[:ï¼š]'
        ]
    
        # åœ¨æ ‡è®°å‰æ’å…¥åˆ†æ®µç¬¦
        for marker in section_markers:
            full_text = re.sub(f'({marker})', r'\n\n\1', full_text)
    
        # å¼ºåˆ¶æ•°å­—åˆ—è¡¨åˆ†æ®µ - å…³é”®ä¿®æ”¹
        # åœ¨ä»»ä½•æ•°å­—åˆ—è¡¨å‰å¼ºåˆ¶åˆ†æ®µï¼Œä¸ç®¡å‰é¢æœ‰æ²¡æœ‰æ ‡ç‚¹ç¬¦å·
        full_text = re.sub(r'(\S)\s*(\d+[\.ã€]\s+)', r'\1\n\n\2', full_text)
    
        # ä¸­æ–‡æ•°å­—åˆ—è¡¨åˆ†æ®µ
        full_text = re.sub(r'(\S)\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][\.ã€]\s+)', r'\1\n\n\2', full_text)
    
        # å¤„ç†å†’å·åçš„æ•°å­—åˆ—è¡¨
        full_text = re.sub(r'([:ï¼š])\s*(\d+[\.ã€]\s+)', r'\1\n\n\2', full_text)
    
        # ç°åœ¨æŒ‰åŒæ¢è¡Œåˆ†å‰²æ®µè½
        paragraphs = re.split(r'\n\n+', full_text)
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
    
        # è¿›ä¸€æ­¥å¤„ç†æ¯ä¸ªæ®µè½å†…éƒ¨çš„å¥å­
        final_paragraphs = []
    
        for paragraph in paragraphs:
            # å¦‚æœæ®µè½ä»¥æ•°å­—å¼€å¤´ï¼Œä¿æŒç‹¬ç«‹
            if re.match(r'^\d+[\.ã€]\s+', paragraph):
                final_paragraphs.append(paragraph)
                continue
        
            # ä¿®å¤ï¼šæ­£ç¡®æ£€æŸ¥æ®µè½ä¸­æ˜¯å¦åŒ…å«å¤šä¸ªæ•°å­—åˆ—è¡¨é¡¹
            list_matches = re.findall(r'\d+[\.ã€]\s+', paragraph)
            if len(list_matches) >= 2:
                # æŒ‰æ•°å­—åˆ—è¡¨æ‹†åˆ†
                parts = re.split(r'(\d+[\.ã€]\s+)', paragraph)
                current_part = ""
            
                for i, part in enumerate(parts):
                    if re.match(r'^\d+[\.ã€]\s+$', part):
                        # è¿™æ˜¯æ•°å­—æ ‡è®°
                        if current_part.strip():
                            final_paragraphs.append(current_part.strip())
                        current_part = part
                    else:
                        current_part += part
            
                if current_part.strip():
                    final_paragraphs.append(current_part.strip())
            else:
                final_paragraphs.append(paragraph)
    
        # è¿”å›åˆ†æ®µç»“æœ
        result = '\n\n'.join(final_paragraphs)
    
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        result = re.sub(r'\n{3,}', '\n\n', result)
    
        return result

    def _final_format_processing(self, text: str) -> str:
        """
        æœ€ç»ˆæ ¼å¼åŒ–å¤„ç†
    
        Args:
            text: å¾…å¤„ç†çš„æ–‡æœ¬
        
        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        # åˆ†å‰²æ®µè½
        paragraphs = text.split('\n\n')
        processed_paragraphs = []
    
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # å¤„ç†åˆ—è¡¨é¡¹çš„ç¼©è¿›å’Œæ ¼å¼
            lines = paragraph.split('\n')
            formatted_lines = []
        
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ä¸ºæ•°å­—åˆ—è¡¨å’Œä¸­æ–‡åˆ—è¡¨æ·»åŠ é€‚å½“çš„æ ¼å¼
                if re.match(r'^[\d]+[ã€.]', line):
                    formatted_lines.append(line)
                elif re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ã€.]', line):
                    formatted_lines.append(line)
                else:
                    formatted_lines.append(line)
        
            if formatted_lines:
                processed_paragraphs.append('\n'.join(formatted_lines))
    
        # ç¡®ä¿æ®µè½é—´æœ‰é€‚å½“é—´è·
        result = '\n\n'.join(processed_paragraphs)
    
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        result = re.sub(r'\n{3,}', '\n\n', result)
    
        return result.strip()
    
    def process_query(self, user_query: str, 
                     chat_history: Optional[List[Dict]] = None,
                     use_cache: bool = True) -> GDMRAGResponse:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ - ä¸»è¦æ–¹æ³•
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            chat_history: å¯¹è¯å†å²
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            GDM RAGå“åº”ç»“æœ
        """
        start_time = time.time()
        logger.info(f"ğŸ” å¤„ç†æŸ¥è¯¢: {user_query}")
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and self.enable_cache:
            cache_key = self._generate_cache_key(user_query)
            if cache_key in self.cache:
                cached_response = self.cache[cache_key]
                logger.info("ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ")
                return cached_response
        
        try:
            # 1. æŸ¥è¯¢åˆ†ç±»
            query_type = self._simple_query_classification(user_query)
            logger.info(f"ğŸ“‹ æŸ¥è¯¢ç±»å‹: {query_type}")
            
            # 2. æ··åˆæ£€ç´¢ - ä½¿ç”¨HybridRetriever.retrieveæ–¹æ³•
            logger.info("ğŸ” æ‰§è¡Œæ··åˆæ£€ç´¢...")
            retrieval_result = self.hybrid_retriever.retrieve(user_query, top_k=5)
            
            if not retrieval_result.combined_context.strip():
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡")
                return self._create_empty_response(user_query, start_time, "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
            
            logger.info(f"âœ… æ£€ç´¢å®Œæˆ: {retrieval_result.search_strategy}, å¾—åˆ†: {retrieval_result.final_score:.3f}")
            
            # 3. ç”Ÿæˆæç¤ºè¯ - ä½¿ç”¨GraphRAGPromptInterface
            logger.info("ğŸ“ ç”Ÿæˆæç¤ºè¯...")
            prompt_result = self.prompt_interface.create_prompt(
                query=user_query,
                semantic_results=retrieval_result.semantic_results,
                graph_results=retrieval_result.graph_results,
                query_type=query_type,
                fusion_method=retrieval_result.fusion_method,
                chat_history=self._format_chat_history(chat_history)
            )
            
            if not prompt_result['is_valid']:
                logger.warning("âš ï¸ æç¤ºè¯è´¨é‡é—®é¢˜")
            
            # 4. è°ƒç”¨DeepSeekç”Ÿæˆå›ç­”
            logger.info("ğŸ¤– è°ƒç”¨DeepSeekç”Ÿæˆå›ç­”...")
            messages = [{"role": "user", "content": prompt_result['prompt']}]
            deepseek_response = self.deepseek_client.chat_completion(
                messages=messages,
                temperature=0.1,
                max_tokens=1000
            )
            
            if not deepseek_response or 'choices' not in deepseek_response:
                logger.error("âŒ DeepSeekå“åº”æ ¼å¼é”™è¯¯")
                return self._create_error_response(user_query, start_time, "AIå›ç­”ç”Ÿæˆå¤±è´¥")
            
            raw_answer = deepseek_response['choices'][0]['message']['content']
            
            # 5. åå¤„ç†å›ç­”
            final_answer = self._post_process_answer(raw_answer, user_query)
            
            # 6. æå–ä¿¡æ¯æ¥æº
            sources = self._extract_sources(retrieval_result)
            
            # 7. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence_score(retrieval_result)
            
            # 8. æ„å»ºå“åº”
            response = GDMRAGResponse(
                question=user_query,
                answer=final_answer,
                sources=sources,
                context_used=retrieval_result.combined_context,
                response_time=time.time() - start_time,
                confidence_score=confidence,
                retrieval_stats={
                    "search_strategy": retrieval_result.search_strategy,
                    "fusion_method": retrieval_result.fusion_method,
                    "semantic_results_count": len(retrieval_result.semantic_results),
                    "graph_results_count": len(retrieval_result.graph_results),
                    "final_score": retrieval_result.final_score,
                    "total_retrieval_time": retrieval_result.total_retrieval_time,
                    "prompt_quality_score": prompt_result.get('quality_score', 0)
                },
                query_analysis={
                    "query_type": query_type,
                    "context_length": len(retrieval_result.combined_context),
                    "prompt_length": len(prompt_result['prompt']),
                    "estimated_tokens": prompt_result['metrics']['estimated_tokens']
                }
            )
            
            # 9. ç¼“å­˜ç»“æœ
            if use_cache and self.enable_cache:
                self.cache[cache_key] = response
            
            logger.info(f"âœ… æŸ¥è¯¢å¤„ç†å®Œæˆ (è€—æ—¶: {response.response_time:.2f}s, ç½®ä¿¡åº¦: {confidence:.3f})")
            return response
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return self._create_error_response(user_query, start_time, str(e))
    
    def _format_chat_history(self, chat_history: Optional[List[Dict]]) -> Optional[str]:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        if not chat_history:
            return None
        
        history_parts = []
        for i, turn in enumerate(chat_history[-3:], 1):  # åªä¿ç•™æœ€è¿‘3è½®
            role = "ç”¨æˆ·" if turn.get("role") == "user" else "åŠ©æ‰‹"
            content = turn.get("content", "")[:150]  # é™åˆ¶é•¿åº¦
            history_parts.append(f"ç¬¬{i}è½® {role}: {content}")
        
        return "\n".join(history_parts)
    
    def _create_empty_response(self, question: str, start_time: float, reason: str) -> GDMRAGResponse:
        """åˆ›å»ºç©ºå“åº”"""
        return GDMRAGResponse(
            question=question,
            answer="æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„åŒ»å­¦ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚å»ºè®®æ‚¨ï¼š\n1. å°è¯•æ¢ä¸ªè¯´æ³•é‡æ–°æé—®\n2. å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿè·å–å‡†ç¡®ä¿¡æ¯\n3. æŸ¥é˜…æƒå¨åŒ»å­¦èµ„æ–™",
            sources=[],
            context_used="",
            response_time=time.time() - start_time,
            confidence_score=0.0,
            retrieval_stats={
                "search_strategy": "empty_result",
                "fusion_method": "none",
                "semantic_results_count": 0,
                "graph_results_count": 0,
                "final_score": 0.0,
                "total_retrieval_time": 0.0,
                "prompt_quality_score": 0,
                "reason": reason
            },
            query_analysis={
                "query_type": "unknown",
                "context_length": 0,
                "prompt_length": 0,
                "estimated_tokens": 0
            }
        )
    
    def _create_error_response(self, question: str, start_time: float, error: str) -> GDMRAGResponse:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return GDMRAGResponse(
            question=question,
            answer=f"ç³»ç»Ÿå¤„ç†å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
            sources=[],
            context_used="",
            response_time=time.time() - start_time,
            confidence_score=0.0,
            retrieval_stats={
                "search_strategy": "error",
                "fusion_method": "none",
                "semantic_results_count": 0,
                "graph_results_count": 0,
                "final_score": 0.0,
                "total_retrieval_time": 0.0,
                "prompt_quality_score": 0,
                "error": error
            },
            query_analysis={
                "query_type": "error",
                "context_length": 0,
                "prompt_length": 0,
                "estimated_tokens": 0
            }
        )
    
    def batch_query(self, questions: List[str]) -> List[GDMRAGResponse]:
        """
        æ‰¹é‡æŸ¥è¯¢å¤„ç†
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            
        Returns:
            å“åº”ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å¤„ç† {len(questions)} ä¸ªæŸ¥è¯¢")
        
        responses = []
        for i, question in enumerate(questions, 1):
            logger.info(f"å¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªæŸ¥è¯¢: {question}")
            response = self.process_query(question)
            responses.append(response)
        
        return responses
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
        try:
            # è·å–æ··åˆæ£€ç´¢å™¨ç»Ÿè®¡ä¿¡æ¯
            retrieval_stats = self.hybrid_retriever.get_retrieval_statistics()
            
            status = {
                "engine_status": "æ­£å¸¸è¿è¡Œ",
                "cache_size": len(self.cache) if self.enable_cache else 0,
                "retrieval_system": {
                    "status": "æ­£å¸¸" if self.hybrid_retriever else "å¼‚å¸¸",
                    "semantic_chunks": retrieval_stats['semantic_retriever']['chunks_loaded'],
                    "semantic_embeddings": retrieval_stats['semantic_retriever']['embeddings_loaded'],
                    "graph_connected": retrieval_stats['graph_retriever']['connected'],
                    "current_weights": retrieval_stats['weights']
                },
                "prompt_system": {
                    "status": "æ­£å¸¸" if self.prompt_interface else "å¼‚å¸¸",
                    "max_tokens": self.max_context_length
                },
                "deepseek_client": {
                    "status": "æ­£å¸¸" if self.deepseek_client else "å¼‚å¸¸",
                    "api_key_configured": bool(self.deepseek_client.api_key)
                }
            }
            
            return status
            
        except Exception as e:
            logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
            return {
                "engine_status": "å¼‚å¸¸", 
                "error": str(e),
                "cache_size": len(self.cache) if hasattr(self, 'cache') else 0
            }
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        if self.enable_cache and hasattr(self, 'cache'):
            cache_size = len(self.cache)
            self.cache.clear()
            logger.info(f"ğŸ§¹ å·²æ¸…ç† {cache_size} æ¡ç¼“å­˜")
    
    def update_retrieval_weights(self, semantic_weight: float, graph_weight: float):
        """åŠ¨æ€è°ƒæ•´æ£€ç´¢æƒé‡"""
        try:
            self.hybrid_retriever.update_weights(semantic_weight, graph_weight)
            logger.info(f"âš–ï¸ æ£€ç´¢æƒé‡å·²æ›´æ–°: è¯­ä¹‰={semantic_weight:.2f}, å›¾è°±={graph_weight:.2f}")
        except Exception as e:
            logger.error(f"æ›´æ–°æ£€ç´¢æƒé‡å¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­å¼•æ“"""
        try:
            if hasattr(self, 'hybrid_retriever'):
                self.hybrid_retriever.close()
            
            if hasattr(self, 'deepseek_client') and hasattr(self.deepseek_client, 'close'):
                self.deepseek_client.close()
            
            self.clear_cache()
            logger.info("ğŸ”’ GDM GraphRAGå¼•æ“å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"å…³é—­å¼•æ“æ—¶å‡ºé”™: {e}")

# ===== ä¾¿æ·å‡½æ•° =====

def create_gdm_rag_engine(api_key: str = None, **kwargs) -> GDMGraphRAGEngine:
    """åˆ›å»ºGDM GraphRAGå¼•æ“å®ä¾‹"""
    return GDMGraphRAGEngine(deepseek_api_key=api_key, **kwargs)

# ===== æµ‹è¯•ä»£ç  =====

if __name__ == "__main__":
    print("ğŸš€ GDM GraphRAGä¸»å¼•æ“æµ‹è¯•...")
    
    try:
        # 1. åˆ›å»ºå¼•æ“
        print("\n1ï¸âƒ£ åˆå§‹åŒ–å¼•æ“...")
        engine = GDMGraphRAGEngine()
        
        # 2. æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
        print("\n2ï¸âƒ£ æ£€æŸ¥ç³»ç»ŸçŠ¶æ€...")
        status = engine.get_system_status()
        print("ç³»ç»ŸçŠ¶æ€:")
        for key, value in status.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for sub_key, sub_value in value.items():
                    print(f"    {sub_key}: {sub_value}")
            else:
                print(f"  {key}: {value}")
        
        # 3. æµ‹è¯•å•ä¸ªæŸ¥è¯¢
        print(f"\n3ï¸âƒ£ æµ‹è¯•æŸ¥è¯¢å¤„ç†...")
        test_questions = [
            "ä»€ä¹ˆæ˜¯å¦Šå¨ æœŸç³–å°¿ç—…ï¼Ÿ",
            "å¦Šå¨ æœŸç³–å°¿ç—…æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ",
            "å¦‚ä½•æ²»ç–—å¦Šå¨ æœŸç³–å°¿ç—…ï¼Ÿ",
            "å­•å¦‡è¡€ç³–é«˜æœ‰ä»€ä¹ˆé£é™©ï¼Ÿ"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n--- æµ‹è¯•æŸ¥è¯¢ {i}: {question} ---")
            
            response = engine.process_query(question)
            
            print(f"âœ… å¤„ç†å®Œæˆ:")
            print(f"   å“åº”æ—¶é—´: {response.response_time:.2f}ç§’")
            print(f"   ç½®ä¿¡åº¦: {response.confidence_score:.3f}")
            print(f"   ä¿¡æ¯æ¥æº: {', '.join(response.sources) if response.sources else 'æ— '}")
            print(f"   æŸ¥è¯¢ç±»å‹: {response.query_analysis['query_type']}")
            print(f"   æ£€ç´¢ç­–ç•¥: {response.retrieval_stats['search_strategy']}")
            print(f"   èåˆæ–¹æ³•: {response.retrieval_stats['fusion_method']}")
            print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {response.query_analysis['context_length']}")
            print(f"   å›ç­”é¢„è§ˆ: {response.answer[:150]}...")
        
        # 4. æµ‹è¯•æ‰¹é‡æŸ¥è¯¢
        print(f"\n4ï¸âƒ£ æµ‹è¯•æ‰¹é‡æŸ¥è¯¢...")
        batch_questions = [
            "GDMçš„è¯Šæ–­æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦Šå¨ æœŸç³–å°¿ç—…éœ€è¦æ³¨æ„å“ªäº›é¥®é£Ÿï¼Ÿ"
        ]
        
        batch_responses = engine.batch_query(batch_questions)
        print(f"æ‰¹é‡å¤„ç†å®Œæˆ: {len(batch_responses)} ä¸ªç»“æœ")
        
        for i, response in enumerate(batch_responses, 1):
            print(f"  æ‰¹é‡æŸ¥è¯¢ {i}: è€—æ—¶ {response.response_time:.2f}s, ç½®ä¿¡åº¦ {response.confidence_score:.3f}")
        
        # 5. æµ‹è¯•ç¼“å­˜
        print(f"\n5ï¸âƒ£ æµ‹è¯•ç¼“å­˜æœºåˆ¶...")
        cached_response = engine.process_query(test_questions[0])  # é‡å¤æŸ¥è¯¢
        print(f"ç¼“å­˜æŸ¥è¯¢è€—æ—¶: {cached_response.response_time:.4f}ç§’")
        
        # 6. æµ‹è¯•æƒé‡è°ƒæ•´
        print(f"\n6ï¸âƒ£ æµ‹è¯•æƒé‡è°ƒæ•´...")
        print("è°ƒæ•´æ£€ç´¢æƒé‡: æ›´åé‡å›¾è°±æ£€ç´¢")
        engine.update_retrieval_weights(0.3, 0.7)
        
        adjusted_response = engine.process_query("å¦Šå¨ æœŸç³–å°¿ç—…çš„ç—‡çŠ¶")
        print(f"è°ƒæ•´æƒé‡åæ£€ç´¢ç­–ç•¥: {adjusted_response.retrieval_stats['search_strategy']}")
        
        # 7. æ¸…ç†
        print(f"\n7ï¸âƒ£ æ¸…ç†èµ„æº...")
        engine.close()
        
        print(f"\nâœ… GDM GraphRAGä¸»å¼•æ“æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ‰ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
