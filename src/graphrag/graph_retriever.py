"""
å›¾è°±æ£€ç´¢å™¨ - åŸºäºçŸ¥è¯†å›¾è°±çš„ä¿¡æ¯æ£€ç´¢
æ•´åˆgraph_tool.pyçš„æŸ¥è¯¢åŠŸèƒ½
"""

import os
import sys
import logging
import re
import time
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass

# å¯¼å…¥é¡¹ç›®æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../.."))
sys.path.insert(0, project_root)

from src.knowledge_graph.graph_tool import GraphTool, GraphNode, GraphRelation

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GraphSearchResult:
    """å›¾è°±æœç´¢ç»“æœ """
    entities: List[GraphNode]
    relations: List[GraphRelation]
    context_text: str
    relevance_score: float
    search_keywords: List[str]
    search_strategy: str = "default"  # æœç´¢ç­–ç•¥æ ‡è¯†
    retrieval_time: float = 0.0      # æ£€ç´¢è€—æ—¶

class GraphRetriever:
    """å›¾è°±æ£€ç´¢å™¨ç±» """
    
    def __init__(self, 
                 neo4j_uri: str = "neo4j://127.0.0.1:7687",
                 neo4j_user: str = "neo4j",
                 neo4j_password: str = r"42810916402\Ssnx"):
        """
        åˆå§‹åŒ–å›¾è°±æ£€ç´¢å™¨
        
        Args:
            neo4j_uri: Neo4jè¿æ¥åœ°å€
            neo4j_user: ç”¨æˆ·å
            neo4j_password: å¯†ç 
        """
        self.graph_tool = GraphTool(neo4j_uri, neo4j_user, neo4j_password)
        
        # åŒ»å­¦å…³é”®è¯è¯å…¸
        self.medical_keywords = {
            "ç–¾ç—…": ["ç³–å°¿ç—…", "é«˜è¡€å‹", "å¿ƒè„ç—…", "å¦Šå¨ æœŸç³–å°¿ç—…", "GDM", "å¦Šé«˜ç—‡", "è´«è¡€", "æ„ŸæŸ“"],
            "ç—‡çŠ¶": ["å¤šé¥®", "å¤šå°¿", "å¤šé£Ÿ", "ä½“é‡ä¸‹é™", "ç–²åŠ³", "å¤´ç—›", "æ°´è‚¿", "è›‹ç™½å°¿", "è¡€å‹å‡é«˜"],
            "æ²»ç–—": ["èƒ°å²›ç´ ", "è¿åŠ¨", "é¥®é£Ÿ", "è¯ç‰©", "ç›‘æµ‹", "æ§åˆ¶", "ç®¡ç†", "æ³¨å°„", "å£æœè¯"],
            "æ£€æŸ¥": ["è¡€ç³–", "å°¿ç³–", "ç³–è€é‡", "æ£€æµ‹", "ç­›æŸ¥", "OGTT", "è¡€å‹", "å°¿è›‹ç™½", "Bè¶…"],
            "é£é™©": ["é—ä¼ ", "è‚¥èƒ–", "å¹´é¾„", "å®¶æ—å²", "å­•æœŸ", "é«˜é¾„", "æ—¢å¾€å²", "BMI"],
            "è¥å…»": ["é¥®é£Ÿ", "é£Ÿç‰©", "è¥å…»", "çƒ­é‡", "ç¢³æ°´åŒ–åˆç‰©", "è›‹ç™½è´¨", "è„‚è‚ª", "ç»´ç”Ÿç´ "],
            "å¹¶å‘ç—‡": ["æ—©äº§", "å·¨å¤§å„¿", "ä½è¡€ç³–", "é…®ç—‡", "æ„ŸæŸ“", "ç¾Šæ°´è¿‡å¤š", "èƒå„¿çª˜è¿«"]
        }
        
        # é—®é¢˜ç±»å‹è¯†åˆ«æ¨¡å¼
        self.question_patterns = {
            "ç—‡çŠ¶": ["ç—‡çŠ¶æœ‰å“ªäº›", "æœ‰ä»€ä¹ˆç—‡çŠ¶", "ä»€ä¹ˆç—‡çŠ¶", "ç—‡çŠ¶æ˜¯ä»€ä¹ˆ", "è¡¨ç°ä¸º", "æœ‰å“ªäº›è¡¨ç°"],
            "è¯Šæ–­": ["å¦‚ä½•è¯Šæ–­", "è¯Šæ–­æ–¹æ³•", "æ€ä¹ˆè¯Šæ–­", "å¦‚ä½•æ£€æŸ¥", "æ£€æŸ¥ä»€ä¹ˆ", "è¯Šæ–­æ ‡å‡†"],
            "æ²»ç–—": ["å¦‚ä½•æ²»ç–—", "æ²»ç–—æ–¹æ³•", "æ€ä¹ˆæ²»", "ç”¨ä»€ä¹ˆè¯", "å¦‚ä½•ç®¡ç†", "æ²»ç–—"],
            "åŸå› ": ["ä»€ä¹ˆåŸå› ", "ä¸ºä»€ä¹ˆ", "ç—…å› ", "å¼•èµ·", "å¯¼è‡´", "åŸå› "],
            "é¢„é˜²": ["å¦‚ä½•é¢„é˜²", "é¢„é˜²æ–¹æ³•", "æ€æ ·é¿å…", "é˜²æ­¢", "é¢„é˜²æªæ–½"],
            "é¥®é£Ÿ": ["é¥®é£Ÿç®¡ç†", "åƒä»€ä¹ˆ", "é¥®é£Ÿ", "é£Ÿç‰©", "è¥å…»", "ä¸èƒ½åƒ", "åº”è¯¥åƒ"],
            "é£é™©": ["ä»€ä¹ˆé£é™©", "æœ‰ä»€ä¹ˆé£é™©", "å±é™©å› ç´ ", "é£é™©", "é«˜å±", "å®¹æ˜“å¾—", "æ˜“æ‚£"]
        }
        
        logger.info("âœ… å›¾è°±æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_medical_entities(self, query: str) -> Tuple[List[str], str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–åŒ»å­¦å®ä½“å…³é”®è¯
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            (æå–çš„åŒ»å­¦å…³é”®è¯åˆ—è¡¨, é—®é¢˜ç±»å‹)
        """
        entities = []
        query_lower = query.lower()
        
        # 1. è¯†åˆ«é—®é¢˜ç±»å‹
        question_type = "general"
        for q_type, patterns in self.question_patterns.items():
            if any(pattern in query for pattern in patterns):  # ç›´æ¥åœ¨åŸqueryä¸­åŒ¹é…
                question_type = q_type
                break
        
        # 2. é¢„å®šä¹‰å…³é”®è¯ç²¾ç¡®åŒ¹é…
        for category, keywords in self.medical_keywords.items():
            for keyword in keywords:
                if keyword in query:  # ä¸è½¬å°å†™ï¼Œä¿æŒç²¾ç¡®åŒ¹é…
                    entities.append(keyword)
        
        # 3. åŒ»å­¦æœ¯è¯­æ¨¡å¼åŒ¹é… - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼ï¼Œé¿å…åŒ¹é…æ•´å¥
        medical_patterns = [
            r'å¦Šå¨ æœŸç³–å°¿ç—…',
            r'(?<!å¦Šå¨ æœŸ)ç³–å°¿ç—…(?!çš„ç—‡çŠ¶æœ‰å“ªäº›|å¦‚ä½•è¯Šæ–­)',  # é¿å…åŒ¹é…å®Œæ•´é—®å¥
            r'é«˜è¡€å‹', 
            r'è¡€ç³–(?!é«˜æœ‰ä»€ä¹ˆé£é™©)',  # é¿å…åŒ¹é…é—®å¥
            r'èƒ°å²›ç´ ',
            r'ç³–è€é‡(?!æ£€æŸ¥æ€ä¹ˆåš)',  # é¿å…åŒ¹é…é—®å¥
            r'OGTT'
        ]
        
        for pattern in medical_patterns:
            matches = re.findall(pattern, query)
            entities.extend(matches)
        
        # 4. é¿å…æå–å®Œæ•´å¥å­,åªä¿ç•™æœ‰æ„ä¹‰çš„åŒ»å­¦è¯æ±‡
        if not entities:
            # æå–2-4å­—çš„ä¸­æ–‡åŒ»å­¦æœ¯è¯­
            words = re.findall(r'[\u4e00-\u9fff]{2,4}', query)
            stopwords = {"ä»€ä¹ˆ", "å“ªäº›", "å¦‚ä½•", "æ€ä¹ˆ", "æ€æ ·", "ä¸ºä»€ä¹ˆ", "æœ‰ä»€ä¹ˆ", "æ˜¯ä»€ä¹ˆ", "ç—‡çŠ¶", "æ²»ç–—"}
            entities = [w for w in words if w not in stopwords and len(w) >= 2]
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡ï¼Œé¿å…å…³é”®è¯è¿‡å¤š
        unique_entities = list(dict.fromkeys(entities))[:5]  # æœ€å¤šä¿ç•™5ä¸ªå…³é”®è¯
        
        return unique_entities, question_type
    
    def search_entities(self, keywords: List[str]) -> List[GraphNode]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢ç›¸å…³å®ä½“
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            
        Returns:
            åŒ¹é…çš„å®ä½“åˆ—è¡¨
        """
        all_entities = []
        
        for keyword in keywords:
            # 1. ç²¾ç¡®åŒ¹é…
            exact_entities = self.graph_tool.find_entity_by_name(keyword, fuzzy=False)
            all_entities.extend(exact_entities)
            
            # 2. æ¨¡ç³ŠåŒ¹é…
            fuzzy_entities = self.graph_tool.find_entity_by_name(keyword, fuzzy=True)
            all_entities.extend(fuzzy_entities)
            
            # 3. å¦‚æœæ˜¯æ ¸å¿ƒåŒ»å­¦æœ¯è¯­ï¼Œæ‰©å±•æœç´¢
            if any(keyword in keywords for keywords in self.medical_keywords.values()):
                expanded_entities = self._expand_medical_search(keyword)
                all_entities.extend(expanded_entities)
        
        # å»é‡ï¼ˆåŸºäºå®ä½“IDï¼‰
        unique_entities = {}
        for entity in all_entities:
            if entity.id not in unique_entities:
                unique_entities[entity.id] = entity
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        sorted_entities = self._rank_entities_by_relevance(list(unique_entities.values()), keywords)
        
        return sorted_entities[:20]  # é™åˆ¶è¿”å›æ•°é‡
    
    def _expand_medical_search(self, keyword: str) -> List[GraphNode]:
        """
        æ‰©å±•åŒ»å­¦æœ¯è¯­æœç´¢
        
        Args:
            keyword: åŒ»å­¦å…³é”®è¯
            
        Returns:
            æ‰©å±•æœç´¢çš„å®ä½“åˆ—è¡¨
        """
        expanded_entities = []
        
        # ç–¾ç—…æ‰©å±•æ˜ å°„
        disease_expansions = {
            "ç³–å°¿ç—…": ["å¦Šå¨ æœŸç³–å°¿ç—…", "2å‹ç³–å°¿ç—…", "1å‹ç³–å°¿ç—…"],
            "é«˜è¡€å‹": ["å¦Šå¨ æœŸé«˜è¡€å‹", "å¦Šé«˜ç—‡"],
            "æ„ŸæŸ“": ["æ³Œå°¿ç³»æ„ŸæŸ“", "å‘¼å¸é“æ„ŸæŸ“"]
        }
        
        # ç—‡çŠ¶æ‰©å±•æ˜ å°„  
        symptom_expansions = {
            "å¤šé¥®": ["çƒ¦æ¸´", "å£å¹²"],
            "å¤šå°¿": ["å°¿é¢‘", "å¤œå°¿å¢å¤š"],
            "ç–²åŠ³": ["ä¹åŠ›", "ç–²ä¹"]
        }
        
        # æ£€æŸ¥æ‰©å±•æ˜ å°„
        test_expansions = {
            "è¡€ç³–": ["ç©ºè…¹è¡€ç³–", "é¤åè¡€ç³–", "éšæœºè¡€ç³–"],
            "ç³–è€é‡": ["OGTT", "è‘¡è„ç³–è€é‡è¯•éªŒ"]
        }
        
        all_expansions = {**disease_expansions, **symptom_expansions, **test_expansions}
        
        if keyword in all_expansions:
            for expansion in all_expansions[keyword]:
                entities = self.graph_tool.find_entity_by_name(expansion, fuzzy=True)
                expanded_entities.extend(entities)
        
        return expanded_entities
    
    def _rank_entities_by_relevance(self, entities: List[GraphNode], keywords: List[str]) -> List[GraphNode]:
        """
        æŒ‰ç›¸å…³æ€§å¯¹å®ä½“æ’åº
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            keywords: æŸ¥è¯¢å…³é”®è¯
            
        Returns:
            æ’åºåçš„å®ä½“åˆ—è¡¨
        """
        def calculate_relevance(entity: GraphNode) -> float:
            score = 0.0
            entity_name_lower = entity.name.lower()
            
            # 1. åç§°åŒ¹é…å¾—åˆ†
            for keyword in keywords:
                keyword_lower = keyword.lower()
                if keyword_lower == entity_name_lower:
                    score += 10  # å®Œå…¨åŒ¹é…
                elif keyword_lower in entity_name_lower:
                    score += 5   # åŒ…å«åŒ¹é…
                elif entity_name_lower in keyword_lower:
                    score += 3   # è¢«åŒ…å«åŒ¹é…
            
            # 2. å®ä½“ç±»å‹å¾—åˆ†
            important_types = ["Disease", "Symptom", "Treatment", "DiagnosticMethod"]
            if entity.label in important_types:
                score += 2
            
            # 3. å±æ€§åŒ¹é…å¾—åˆ†
            if entity.properties:
                for key, value in entity.properties.items():
                    if isinstance(value, str):
                        for keyword in keywords:
                            if keyword.lower() in value.lower():
                                score += 1
            
            return score
        
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†å¹¶æ’åº
        entity_scores = [(entity, calculate_relevance(entity)) for entity in entities]
        entity_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [entity for entity, score in entity_scores]
    
    def get_entity_context(self, entities: List[GraphNode], question_type: str, max_depth: int = 2) -> Tuple[List[GraphRelation], str]:
        """
        è·å–å®ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            question_type: é—®é¢˜ç±»å‹
            max_depth: æœ€å¤§æœç´¢æ·±åº¦
            
        Returns:
            (å…³ç³»åˆ—è¡¨, ä¸Šä¸‹æ–‡æ–‡æœ¬)
        """
        all_relations = []
        context_parts = []
        processed_entities = set()
        
        # æ ¹æ®é—®é¢˜ç±»å‹ä½¿ç”¨ä¸åŒçš„ä¸Šä¸‹æ–‡è·å–ç­–ç•¥
        for entity in entities[:5]:  # é™åˆ¶å¤„ç†çš„å®ä½“æ•°é‡
            if entity.id in processed_entities:
                continue
                
            processed_entities.add(entity.id)
            
            # ä½¿ç”¨graph_toolçš„é—®é¢˜ä¸Šä¸‹æ–‡æ–¹æ³•
            if question_type != "general":
                question_context = self.graph_tool.get_question_context(f"{entity.name}çš„{question_type}")
                if question_context and "æœªæ‰¾åˆ°" not in question_context:
                    context_parts.append(f"ã€{entity.label}ã€‘{entity.name}\n{question_context}")
                    continue
            
            # è·å–å®ä½“é‚»å±…ä¿¡æ¯ä½œä¸ºè¡¥å……
            neighbors = self.graph_tool.get_entity_neighbors(entity.name)
            
            if neighbors["center"]:
                # æ·»åŠ å®ä½“ä¿¡æ¯
                entity_info = f"ã€{entity.label}ã€‘{entity.name}"
                if entity.properties:
                    props = []
                    for key, value in entity.properties.items():
                        if key not in ['id', 'name'] and value:
                            props.append(f"{key}: {value}")
                    if props:
                        entity_info += f"\nå±æ€§: {', '.join(props[:3])}"
                
                context_parts.append(entity_info)
                
                # æ·»åŠ ç›¸å…³å…³ç³»ä¿¡æ¯
                if neighbors["all"]:
                    relation_texts = []
                    for neighbor in neighbors["all"][:5]:  # é™åˆ¶æ¯ä¸ªå®ä½“çš„é‚»å±…æ•°é‡
                        relation_text = f"{neighbor['relation']} {neighbor['name']}"
                        relation_texts.append(relation_text)
                        
                        # åˆ›å»ºå…³ç³»å¯¹è±¡
                        relation = GraphRelation(
                            source=entity.name,
                            target=neighbor['name'],
                            relation_type=neighbor['relation'],
                            properties={}
                        )
                        all_relations.append(relation)
                    
                    if relation_texts:
                        context_parts.append(f"ç›¸å…³: {', '.join(relation_texts)}")
        
        context_text = "\n\n".join(context_parts)
        return all_relations, context_text
    
    def calculate_relevance_score(self, query: str, entities: List[GraphNode], 
                                relations: List[GraphRelation], question_type: str) -> float:
        """
        è®¡ç®—æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§å¾—åˆ†
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            entities: åŒ¹é…çš„å®ä½“
            relations: ç›¸å…³å…³ç³»
            question_type: é—®é¢˜ç±»å‹
            
        Returns:
            ç›¸å…³æ€§å¾—åˆ† (0-1)
        """
        if not entities:
            return 0.0
        
        score = 0.0
        query_lower = query.lower()
        
        # 1. å®ä½“åç§°åŒ¹é…å¾—åˆ†,æé«˜åŸºç¡€åˆ†å€¼
        entity_score = 0.0
        for entity in entities:
            entity_name_lower = entity.name.lower()
            
            # å®Œå…¨åŒ¹é…æˆ–åŒ…å«åŒ¹é…
            if entity_name_lower in query_lower or query_lower.replace("çš„", "").replace("ï¼Ÿ", "") in entity_name_lower:
                entity_score += 0.4  # æé«˜åŸºç¡€åŒ¹é…åˆ†å€¼
            
            # æ ¸å¿ƒåŒ»å­¦æœ¯è¯­åŒ¹é…
            key_terms = ["å¦Šå¨ æœŸç³–å°¿ç—…", "ç³–å°¿ç—…", "è¡€ç³–", "èƒ°å²›ç´ ", "ç³–è€é‡"]
            for term in key_terms:
                if term in entity.name and term in query:
                    entity_score += 0.3  # åŒ»å­¦æœ¯è¯­åŒ¹é…åŠ åˆ†
            
            # éƒ¨åˆ†è¯åŒ¹é…
            entity_words = set(re.findall(r'[\u4e00-\u9fff]{2,}', entity.name))
            query_words = set(re.findall(r'[\u4e00-\u9fff]{2,}', query))
            common_words = entity_words & query_words
            if common_words:
                entity_score += len(common_words) * 0.1
        
        # 2. é—®é¢˜ç±»å‹åŒ¹é…å¾—åˆ†
        type_score = 0.0
        type_mapping = {
            "ç—‡çŠ¶": ["Symptom", "MedicalConcept"],
            "æ²»ç–—": ["Treatment", "Medication"],
            "è¯Šæ–­": ["DiagnosticMethod"], 
            "é£é™©": ["RiskFactor", "Complication"],
            "é¥®é£Ÿ": ["Food", "NutritionalGuideline"],
            "general": ["Disease", "Symptom", "Treatment", "DiagnosticMethod"]
        }
        
        expected_types = type_mapping.get(question_type, type_mapping["general"])
        type_match_count = 0
        for entity in entities:
            if entity.label in expected_types:
                type_match_count += 1
                type_score += 0.2  # æ¯ä¸ªç±»å‹åŒ¹é…åŠ 0.2åˆ†
        
        # 3. å…³ç³»ä¸°å¯Œåº¦å¾—åˆ† - è°ƒæ•´æƒé‡
        relation_score = min(len(relations) * 0.03, 0.2)  # æ¯ä¸ªå…³ç³»0.03åˆ†ï¼Œæœ€å¤š0.2åˆ†
        
        # 4. ä¸Šä¸‹æ–‡è´¨é‡å¾—åˆ†
        context_score = 0.0
        for entity in entities:
            if entity.properties:
                # æœ‰descriptionå±æ€§çš„å®ä½“åŠ åˆ†æ›´å¤š
                if 'description' in entity.properties:
                    context_score += 0.15
                else:
                    context_score += 0.05
        context_score = min(context_score, 0.25)  # æœ€å¤š0.25åˆ†
        
        # 5. ç–¾ç—…ä¸“ç”¨æŸ¥è¯¢åŠ æˆ
        disease_bonus = 0.0
        if any("å¦Šå¨ æœŸç³–å°¿ç—…" in entity.name for entity in entities):
            disease_bonus = 0.2  # æ‰¾åˆ°æ ¸å¿ƒç–¾ç—…æ—¶åŠ æˆ
        
        # ç»¼åˆè®¡ç®—
        total_score = entity_score + type_score + relation_score + context_score + disease_bonus
        
        # ç»“æœè´¨é‡åŠ æˆ
        quality_multiplier = 1.0
        if len(entities) >= 3 and len(relations) >= 5:
            quality_multiplier = 1.1  # ç»“æœä¸°å¯Œæ—¶è½»å¾®åŠ æˆ
        elif type_match_count >= 2:
            quality_multiplier = 1.05  # ç±»å‹åŒ¹é…å¥½æ—¶åŠ æˆ
        
        # æœ€ç»ˆåˆ†æ•°è®¡ç®—å’Œå½’ä¸€åŒ–
        final_score = total_score * quality_multiplier
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†… (0.1-1.0)
        if final_score > 0:
            final_score = max(final_score, 0.1)  # æœ‰ç»“æœæ—¶æœ€ä½0.1åˆ†
            final_score = min(final_score, 1.0)   # æœ€é«˜1.0åˆ†
        
        return final_score
    
    def retrieve(self, query: str, top_k: int = 5) -> List[GraphSearchResult]:
        """
        æ‰§è¡Œå›¾è°±æ£€ç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            å›¾è°±æœç´¢ç»“æœåˆ—è¡¨
        """
        start_time = time.time()
        logger.info(f"ğŸ” å›¾è°±æ£€ç´¢æŸ¥è¯¢: {query}")
        
        try:
            # 1. æå–åŒ»å­¦å®ä½“å…³é”®è¯å’Œé—®é¢˜ç±»å‹
            keywords, question_type = self.extract_medical_entities(query)
            logger.info(f"æå–å…³é”®è¯: {keywords}, é—®é¢˜ç±»å‹: {question_type}")
            
            if not keywords:
                # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œä½¿ç”¨æŸ¥è¯¢ä¸­çš„æ‰€æœ‰æœ‰æ„ä¹‰çš„è¯
                keywords = [word.strip() for word in query.split() if len(word.strip()) > 1]
            
            # 2. æœç´¢ç›¸å…³å®ä½“
            entities = self.search_entities(keywords)
            logger.info(f"æ‰¾åˆ° {len(entities)} ä¸ªç›¸å…³å®ä½“")
            
            if not entities:
                retrieval_time = time.time() - start_time
                return [GraphSearchResult(
                    entities=[],
                    relations=[],
                    context_text="æœªæ‰¾åˆ°ç›¸å…³çš„å›¾è°±ä¿¡æ¯",
                    relevance_score=0.0,
                    search_keywords=keywords,
                    search_strategy="empty_result",
                    retrieval_time=retrieval_time
                )]
            
            # 3. è·å–å®ä½“ä¸Šä¸‹æ–‡
            relations, context_text = self.get_entity_context(entities, question_type)
            
            # 4. è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
            relevance_score = self.calculate_relevance_score(query, entities, relations, question_type)
            
            # 5. ç¡®å®šæœç´¢ç­–ç•¥
            if question_type != "general":
                search_strategy = f"specialized_{question_type}"
            else:
                search_strategy = "general_graph"
            
            retrieval_time = time.time() - start_time
            
            # 6. æ„å»ºæœç´¢ç»“æœ
            result = GraphSearchResult(
                entities=entities[:top_k],  # é™åˆ¶å®ä½“æ•°é‡
                relations=relations,
                context_text=context_text,
                relevance_score=relevance_score,
                search_keywords=keywords,
                search_strategy=search_strategy,
                retrieval_time=retrieval_time
            )
            
            logger.info(f"âœ… å›¾è°±æ£€ç´¢å®Œæˆï¼Œè€—æ—¶: {retrieval_time:.3f}sï¼Œç›¸å…³æ€§: {relevance_score:.3f}")
            return [result]  # è¿”å›å•ä¸ªå¢å¼ºç»“æœ
            
        except Exception as e:
            logger.error(f"âŒ å›¾è°±æ£€ç´¢å¤±è´¥: {e}")
            retrieval_time = time.time() - start_time
            return [GraphSearchResult(
                entities=[],
                relations=[],
                context_text=f"å›¾è°±æ£€ç´¢å‡ºç°é”™è¯¯: {str(e)}",
                relevance_score=0.0,
                search_keywords=keywords if 'keywords' in locals() else [],
                search_strategy="error",
                retrieval_time=retrieval_time
            )]
    
    def get_disease_context(self, disease_name: str) -> Optional[GraphSearchResult]:
        """
        è·å–ç‰¹å®šç–¾ç—…çš„è¯¦ç»†ä¸Šä¸‹æ–‡
        
        Args:
            disease_name: ç–¾ç—…åç§°
            
        Returns:
            ç–¾ç—…ç›¸å…³çš„å›¾è°±ä¿¡æ¯
        """
        start_time = time.time()
        
        try:
            # ä½¿ç”¨graph_toolçš„ä¸“ç”¨ç–¾ç—…æŸ¥è¯¢æ–¹æ³•
            disease_info = self.graph_tool.get_disease_info(disease_name)
            
            if not disease_info:
                return None
            
            # æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡æ–‡æœ¬
            context_parts = [f"ã€ç–¾ç—…è¯¦æƒ…ã€‘{disease_info['name']}"]
            
            if disease_info.get('symptoms'):
                symptoms = disease_info['symptoms'][:8]  # é™åˆ¶æ•°é‡
                context_parts.append(f"ä¸»è¦ç—‡çŠ¶: {', '.join(symptoms)}")
            
            if disease_info.get('treatments'):
                treatments = disease_info['treatments'][:5]
                context_parts.append(f"æ²»ç–—æ–¹æ³•: {', '.join(treatments)}")
            
            if disease_info.get('risk_factors'):
                risks = disease_info['risk_factors'][:5]
                context_parts.append(f"é£é™©å› ç´ : {', '.join(risks)}")
            
            if disease_info.get('diagnosis_methods'):
                diagnosis = disease_info['diagnosis_methods'][:5]
                context_parts.append(f"è¯Šæ–­æ–¹æ³•: {', '.join(diagnosis)}")
            
            if disease_info.get('complications'):
                complications = disease_info['complications'][:5]
                context_parts.append(f"å¯èƒ½å¹¶å‘ç—‡: {', '.join(complications)}")
            
            context_text = "\n".join(context_parts)
            
            # æ„å»ºå®ä½“å¯¹è±¡
            entities = [GraphNode(
                id="disease_" + disease_name,
                name=disease_name,
                label="Disease",
                properties=disease_info.get('properties', {})
            )]
            
            retrieval_time = time.time() - start_time
            
            return GraphSearchResult(
                entities=entities,
                relations=[],  # å…³ç³»ä¿¡æ¯å·²ç»æ•´åˆåˆ°context_textä¸­
                context_text=context_text,
                relevance_score=1.0,  # ç›´æ¥ç–¾ç—…æŸ¥è¯¢ï¼Œç›¸å…³æ€§æœ€é«˜
                search_keywords=[disease_name],
                search_strategy="disease_specific",
                retrieval_time=retrieval_time
            )
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç–¾ç—…ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return None
    
    def close(self):
        """å…³é—­å›¾è°±è¿æ¥"""
        if hasattr(self.graph_tool, 'close'):
            self.graph_tool.close()
        logger.info("âœ… å›¾è°±æ£€ç´¢å™¨å·²å…³é—­")

# ä¾¿æ·å‡½æ•°
def create_graph_retriever(**kwargs) -> GraphRetriever:
    """åˆ›å»ºå›¾è°±æ£€ç´¢å™¨å®ä¾‹"""
    return GraphRetriever(**kwargs)

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸš€ æµ‹è¯•å›¾è°±æ£€ç´¢å™¨...")
    
    try:
        retriever = GraphRetriever()
        
        # 1ï¸âƒ£ æµ‹è¯•å…³é”®è¯æå–
        print("\n1ï¸âƒ£ æµ‹è¯•å…³é”®è¯æå–...")
        test_queries_for_extraction = [
            "å¦Šå¨ æœŸç³–å°¿ç—…æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ",
            "å¦‚ä½•è¯Šæ–­ç³–å°¿ç—…ï¼Ÿ",
            "å­•å¦‡è¡€ç³–é«˜æœ‰ä»€ä¹ˆé£é™©ï¼Ÿ",
            "ç³–è€é‡æ£€æŸ¥æ€ä¹ˆåšï¼Ÿ",
            "å¦Šå¨ æœŸç³–å°¿ç—…çš„é¥®é£Ÿç®¡ç†"
        ]
        
        for query in test_queries_for_extraction:
            keywords, question_type = retriever.extract_medical_entities(query)
            print(f"æŸ¥è¯¢: {query}")
            print(f"  â¤ æå–å…³é”®è¯: {keywords}")
            print(f"  â¤ é—®é¢˜ç±»å‹: {question_type}")
            print()
        
        # 2ï¸âƒ£ æµ‹è¯•å›¾è°±æ£€ç´¢
        print("\n2ï¸âƒ£ æµ‹è¯•å›¾è°±æ£€ç´¢...")
        test_queries = [
            "å¦Šå¨ æœŸç³–å°¿ç—…çš„ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ",
            "å¦‚ä½•æ²»ç–—ç³–å°¿ç—…ï¼Ÿ", 
            "å­•å¦‡è¡€ç³–é«˜æœ‰ä»€ä¹ˆé£é™©ï¼Ÿ",
            "ç³–è€é‡æ£€æŸ¥æ€ä¹ˆåšï¼Ÿ"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n--- æµ‹è¯•æŸ¥è¯¢ {i}: {query} ---")
            results = retriever.retrieve(query, top_k=3)
            
            if results:
                for j, result in enumerate(results, 1):
                    print(f"\nğŸ“‹ ç»“æœ {j}:")
                    print(f"  ç›¸å…³æ€§å¾—åˆ†: {result.relevance_score:.3f}")
                    print(f"  æœç´¢ç­–ç•¥: {result.search_strategy}")
                    print(f"  æ£€ç´¢è€—æ—¶: {result.retrieval_time:.3f}s")
                    print(f"  æ‰¾åˆ°å®ä½“: {len(result.entities)} ä¸ª")
                    if result.entities:
                        entity_names = [entity.name for entity in result.entities[:3]]
                        print(f"    å®ä½“åˆ—è¡¨: {', '.join(entity_names)}")
                    print(f"  æ‰¾åˆ°å…³ç³»: {len(result.relations)} ä¸ª")
                    print(f"  æœç´¢å…³é”®è¯: {result.search_keywords}")
                    
                    # æ˜¾ç¤ºä¸Šä¸‹æ–‡é¢„è§ˆ
                    if result.context_text:
                        preview = result.context_text[:300] + "..." if len(result.context_text) > 300 else result.context_text
                        print(f"  ä¸Šä¸‹æ–‡é¢„è§ˆ: {preview}")
                    else:
                        print("  ä¸Šä¸‹æ–‡: æ— ")
            else:
                print("  âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        
        # 3ï¸âƒ£ æµ‹è¯•ç–¾ç—…ä¸“ç”¨æŸ¥è¯¢
        print("\n3ï¸âƒ£ æµ‹è¯•ç–¾ç—…ä¸“ç”¨æŸ¥è¯¢...")
        disease_names = [
            "å¦Šå¨ æœŸç³–å°¿ç—…",
            "ç³–å°¿ç—…", 
            "é«˜è¡€å‹",
            "å¦Šå¨ æœŸé«˜è¡€å‹"
        ]
        
        for disease in disease_names:
            print(f"\n--- æµ‹è¯•ç–¾ç—…: {disease} ---")
            disease_result = retriever.get_disease_context(disease)
            
            if disease_result:
                print(f"âœ… æ‰¾åˆ°ç–¾ç—…ä¿¡æ¯:")
                print(f"  ç›¸å…³æ€§å¾—åˆ†: {disease_result.relevance_score:.3f}")
                print(f"  æœç´¢ç­–ç•¥: {disease_result.search_strategy}")
                print(f"  æ£€ç´¢è€—æ—¶: {disease_result.retrieval_time:.3f}s")
                print(f"  å®ä½“æ•°é‡: {len(disease_result.entities)}")
                
                # æ˜¾ç¤ºç–¾ç—…è¯¦ç»†ä¿¡æ¯
                if disease_result.context_text:
                    print(f"  ç–¾ç—…è¯¦æƒ…:")
                    # æŒ‰è¡Œåˆ†å‰²å¹¶æ·»åŠ ç¼©è¿›ï¼Œä¾¿äºé˜…è¯»
                    lines = disease_result.context_text.split('\n')
                    for line in lines[:10]:  # é™åˆ¶æ˜¾ç¤ºè¡Œæ•°
                        if line.strip():
                            print(f"    {line}")
                    if len(lines) > 10:
                        print(f"    ... (è¿˜æœ‰ {len(lines) - 10} è¡Œ)")
                else:
                    print("  ç–¾ç—…è¯¦æƒ…: æ— ")
            else:
                print(f"  âŒ æœªæ‰¾åˆ°ç–¾ç—… '{disease}' çš„ç›¸å…³ä¿¡æ¯")
        
        # 4ï¸âƒ£ æµ‹è¯•å®ä½“æœç´¢åŠŸèƒ½
        print("\n4ï¸âƒ£ æµ‹è¯•å®ä½“æœç´¢...")
        test_entities = ["ç³–å°¿ç—…", "è¡€ç³–", "èƒ°å²›ç´ ", "å­•æœŸ"]
        
        for entity_name in test_entities:
            print(f"\n--- æœç´¢å®ä½“: {entity_name} ---")
            entities = retriever.search_entities([entity_name])
            
            if entities:
                print(f"âœ… æ‰¾åˆ° {len(entities)} ä¸ªç›¸å…³å®ä½“:")
                for i, entity in enumerate(entities[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"  {i}. ã€{entity.label}ã€‘{entity.name}")
                    if entity.properties:
                        # æ˜¾ç¤ºéƒ¨åˆ†å±æ€§
                        props = []
                        for key, value in list(entity.properties.items())[:2]:
                            if key not in ['id', 'name'] and value:
                                props.append(f"{key}: {value}")
                        if props:
                            print(f"     å±æ€§: {', '.join(props)}")
            else:
                print(f"  âŒ æœªæ‰¾åˆ°å®ä½“ '{entity_name}'")
        
        # 5ï¸âƒ£ æ€§èƒ½æµ‹è¯•
        print("\n5ï¸âƒ£ æ€§èƒ½æµ‹è¯•...")
        performance_queries = [
            "å¦Šå¨ æœŸç³–å°¿ç—…",
            "è¡€ç³–ç›‘æµ‹",
            "èƒ°å²›ç´ æ³¨å°„",
            "å­•æœŸè¥å…»ç®¡ç†"
        ]
        
        total_time = 0
        total_queries = len(performance_queries)
        
        for query in performance_queries:
            start_time = time.time()
            results = retriever.retrieve(query, top_k=5)
            query_time = time.time() - start_time
            total_time += query_time
            
            print(f"  æŸ¥è¯¢ '{query}': {query_time:.3f}s, ç»“æœæ•°: {len(results)}")
        
        avg_time = total_time / total_queries
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  æ€»æŸ¥è¯¢æ•°: {total_queries}")
        print(f"  æ€»è€—æ—¶: {total_time:.3f}s")
        print(f"  å¹³å‡è€—æ—¶: {avg_time:.3f}s/æŸ¥è¯¢")
        print(f"  æŸ¥è¯¢æ•ˆç‡: {'ä¼˜ç§€' if avg_time < 0.5 else 'è‰¯å¥½' if avg_time < 1.0 else 'éœ€ä¼˜åŒ–'}")
        
        # æ¸…ç†èµ„æº
        retriever.close()
        print("\nâœ… å›¾è°±æ£€ç´¢å™¨æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
