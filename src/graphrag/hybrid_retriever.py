"""
æ··åˆæ£€ç´¢å™¨ - æ•´åˆè¯­ä¹‰æ£€ç´¢å’Œå›¾è°±æ£€ç´¢
å®ç°GraphRAGçš„æ ¸å¿ƒæ£€ç´¢é€»è¾‘
"""

import os
import sys
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

# å¯¼å…¥é¡¹ç›®æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../.."))
sys.path.insert(0, project_root)

from src.graphrag.embeddings import EmbeddingEngine, DocumentChunk
from src.graphrag.graph_retriever import GraphRetriever, GraphSearchResult

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class HybridSearchResult:
    """æ··åˆæœç´¢ç»“æœ"""
    semantic_results: List[Tuple[DocumentChunk, float]]  # (chunk, similarity_score)
    graph_results: List[GraphSearchResult]
    combined_context: str
    final_score: float
    search_strategy: str
    total_retrieval_time: float = 0.0  # å¢æ€»æ£€ç´¢æ—¶é—´
    fusion_method: str = "default"     # å¢èåˆæ–¹æ³•

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ç±»"""
    
    def __init__(self, 
                 embedding_model: str = "all-mpnet-base-v2",
                 semantic_weight: float = 0.6,
                 graph_weight: float = 0.4,
                 max_semantic_results: int = 5,
                 max_graph_results: int = 3):
        """
        åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
        
        Args:
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            semantic_weight: è¯­ä¹‰æ£€ç´¢æƒé‡
            graph_weight: å›¾è°±æ£€ç´¢æƒé‡
            max_semantic_results: è¯­ä¹‰æ£€ç´¢æœ€å¤§ç»“æœæ•°
            max_graph_results: å›¾è°±æ£€ç´¢æœ€å¤§ç»“æœæ•°
        """
        self.semantic_weight = semantic_weight
        self.graph_weight = graph_weight
        self.max_semantic_results = max_semantic_results
        self.max_graph_results = max_graph_results
        
        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self.embedding_engine = EmbeddingEngine(model_name=embedding_model)
        self.graph_retriever = GraphRetriever()
        
        # æŸ¥è¯¢ç±»å‹æƒé‡é…ç½® - å¢è‡ªé€‚åº”æƒé‡
        self.query_type_weights = {
            "knowledge_based": {"semantic": 0.3, "graph": 0.7},    # çŸ¥è¯†å‹é—®é¢˜åé‡å›¾è°±
            "factual": {"semantic": 0.2, "graph": 0.8},            # äº‹å®å‹é—®é¢˜é‡å›¾è°±
            "contextual": {"semantic": 0.7, "graph": 0.3},         # ä¸Šä¸‹æ–‡å‹é—®é¢˜åé‡å‘é‡
            "general": {"semantic": semantic_weight, "graph": graph_weight}  # é€šç”¨é—®é¢˜ä½¿ç”¨é»˜è®¤æƒé‡
        }
        
        # åŠ è½½å‘é‡æ•°æ®
        self.load_vectors()
        
        logger.info(f"âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ (è¯­ä¹‰æƒé‡: {semantic_weight}, å›¾è°±æƒé‡: {graph_weight})")
    
    def load_vectors(self) -> bool:
        """åŠ è½½å‘é‡æ•°æ® - å¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            success = self.embedding_engine.load_vectors()
            if success:
                logger.info("âœ… å‘é‡æ•°æ®åŠ è½½æˆåŠŸ")
                return True
            else:
                logger.warning("âš ï¸ å‘é‡æ•°æ®åŠ è½½å¤±è´¥ï¼Œå°†å°è¯•é‡æ–°ç”Ÿæˆ")
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°è¯•é‡æ–°å¤„ç†æ•°æ®
                doc_count = self.embedding_engine.process_documents()
                if doc_count > 0:
                    if self.embedding_engine.generate_embeddings():
                        save_path = self.embedding_engine.save_vectors()
                        if save_path:
                            logger.info("âœ… å‘é‡æ•°æ®é‡æ–°ç”Ÿæˆå¹¶ä¿å­˜æˆåŠŸ")
                            return True
                
                logger.error("âŒ å‘é‡æ•°æ®å¤„ç†å®Œå…¨å¤±è´¥")
                return False
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return False
    
    def classify_query_type(self, query: str) -> str:
        """
        åˆ†ç±»æŸ¥è¯¢ç±»å‹ - æ–°å¢æ–¹æ³•
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            æŸ¥è¯¢ç±»å‹
        """
        query_lower = query.lower()
        
        # çŸ¥è¯†å‹é—®é¢˜å…³é”®è¯
        knowledge_keywords = ["ä»€ä¹ˆæ˜¯", "å¦‚ä½•", "æ€æ ·", "ä¸ºä»€ä¹ˆ", "åŸå› ", "æœºåˆ¶", "å®šä¹‰"]
        factual_keywords = ["ç—‡çŠ¶", "æ²»ç–—", "è¯Šæ–­", "æ£€æŸ¥", "è¯ç‰©", "é£é™©", "å¹¶å‘ç—‡"]
        contextual_keywords = ["ç—…ä¾‹", "æ¡ˆä¾‹", "ç»éªŒ", "ç»å†", "æ•…äº‹", "æƒ…å†µ"]
        
        # ç»Ÿè®¡å„ç±»å…³é”®è¯å‡ºç°æ¬¡æ•°
        knowledge_count = sum(1 for keyword in knowledge_keywords if keyword in query_lower)
        factual_count = sum(1 for keyword in factual_keywords if keyword in query_lower)
        contextual_count = sum(1 for keyword in contextual_keywords if keyword in query_lower)
        
        # ç¡®å®šæŸ¥è¯¢ç±»å‹
        if knowledge_count > 0:
            return "knowledge_based"
        elif factual_count > 0:
            return "factual"
        elif contextual_count > 0:
            return "contextual"
        else:
            return "general"
    
    def semantic_retrieve(self, query: str, top_k: int) -> List[Tuple[DocumentChunk, float]]:
        """
        æ‰§è¡Œè¯­ä¹‰æ£€ç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            è¯­ä¹‰æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # æ£€æŸ¥å‘é‡æ•°æ®æ˜¯å¦å·²åŠ è½½
            if not hasattr(self.embedding_engine, 'chunks') or not self.embedding_engine.chunks:
                logger.warning("âš ï¸ æ²¡æœ‰å¯æœç´¢çš„æ–‡æ¡£åˆ†å—")
                return []
            
            if not hasattr(self.embedding_engine, 'vectors') or self.embedding_engine.vectors is None:
                logger.warning("âš ï¸ å‘é‡æ•°æ®æœªç”Ÿæˆæˆ–æœªåŠ è½½")
                return []
            
            # ä½¿ç”¨ EmbeddingEngine çš„ similarity_search æ–¹æ³•
            results = self.embedding_engine.similarity_search(
                query=query, 
                top_k=top_k,
                min_score=0.3  # è®¾ç½®æœ€ä½ç›¸å…³æ€§é˜ˆå€¼ï¼Œè¿‡æ»¤æ— å…³ç»“æœ
            )
            
            logger.info(f"âœ… è¯­ä¹‰æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def graph_retrieve(self, query: str, top_k: int) -> List[GraphSearchResult]:
        """
        æ‰§è¡Œå›¾è°±æ£€ç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            å›¾è°±æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            return self.graph_retriever.retrieve(query, top_k=top_k)
        except Exception as e:
            logger.error(f"âŒ å›¾è°±æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def calculate_combined_score(self, 
                               semantic_results: List[Tuple[DocumentChunk, float]], 
                               graph_results: List[GraphSearchResult],
                               query_type: str) -> float:
        """
        è®¡ç®—ç»¼åˆç›¸å…³æ€§å¾—åˆ†
        
        Args:
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœ
            graph_results: å›¾è°±æ£€ç´¢ç»“æœ
            query_type: æŸ¥è¯¢ç±»å‹
            
        Returns:
            ç»¼åˆç›¸å…³æ€§å¾—åˆ†
        """
        # è·å–åŠ¨æ€æƒé‡
        weights = self.query_type_weights.get(query_type, self.query_type_weights["general"])
        
        # è¯­ä¹‰å¾—åˆ† - ä½¿ç”¨æœ€ä½³åŒ¹é…çš„å¹³å‡å€¼
        semantic_score = 0.0
        if semantic_results:
            top_semantic_scores = [score for _, score in semantic_results[:3]]
            semantic_score = sum(top_semantic_scores) / len(top_semantic_scores)
        
        # å›¾è°±å¾—åˆ† - ä½¿ç”¨åŠ æƒå¹³å‡
        graph_score = 0.0
        if graph_results:
            total_weight = 0
            weighted_sum = 0
            for result in graph_results:
                # æ ¹æ®å®ä½“æ•°é‡ç»™äºˆæƒé‡
                entity_weight = min(len(result.entities), 5) / 5.0 + 0.2  # åŸºç¡€æƒé‡0.2
                weighted_sum += result.relevance_score * entity_weight
                total_weight += entity_weight
            
            if total_weight > 0:
                graph_score = weighted_sum / total_weight
        
        # ç»¼åˆå¾—åˆ†
        combined_score = (semantic_score * weights["semantic"] + 
                         graph_score * weights["graph"])
        
        # ç»“æœè´¨é‡åŠ æˆ
        quality_bonus = 1.0
        if semantic_results and graph_results:
            quality_bonus = 1.2  # ä¸¤ç§æ£€ç´¢éƒ½æœ‰ç»“æœæ—¶åŠ æˆ
        elif len(semantic_results) >= 3 or (graph_results and len(graph_results[0].entities) >= 2):
            quality_bonus = 1.1  # å•ä¸€æ£€ç´¢ç»“æœä¸°å¯Œæ—¶è½»å¾®åŠ æˆ
        
        final_score = min(combined_score * quality_bonus, 1.0)  # é™åˆ¶åœ¨1.0ä»¥å†…
        
        return final_score
    
    def _combine_contexts(self, semantic_context: str, graph_context: str, fusion_method: str) -> str:
        """æ ¹æ®èåˆæ–¹æ³•åˆå¹¶ä¸Šä¸‹æ–‡"""
        if fusion_method == "graph_first":
            return f"{graph_context}\n\n{semantic_context}"
        elif fusion_method == "semantic_first":
            return f"{semantic_context}\n\n{graph_context}"
        else:  # balanced or other
            return f"{graph_context}\n\n{semantic_context}"
    
    def fuse_contexts(self, 
                    semantic_results: List[Tuple[DocumentChunk, float]], 
                    graph_results: List[GraphSearchResult],
                    query_type: str) -> Tuple[str, str]:
        """
        èåˆè¯­ä¹‰å’Œå›¾è°±ä¸Šä¸‹æ–‡
        
        Args:
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœ
            graph_results: å›¾è°±æ£€ç´¢ç»“æœ
            query_type: æŸ¥è¯¢ç±»å‹
            
        Returns:
            (èåˆåçš„ä¸Šä¸‹æ–‡, èåˆæ–¹æ³•)
        """
        context_parts = []
        fusion_method = "default"
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ä¸åŒçš„èåˆç­–ç•¥
        if query_type == "factual" and graph_results:
            # äº‹å®å‹é—®é¢˜ï¼šä¼˜å…ˆå±•ç¤ºå›¾è°±ç»“æ„åŒ–ä¿¡æ¯
            fusion_method = "graph_first"
            for i, graph_result in enumerate(graph_results):
                if graph_result.context_text and "æœªæ‰¾åˆ°" not in graph_result.context_text:
                    context_parts.append(f"ã€çŸ¥è¯†å›¾è°±-{i+1}ã€‘\n{graph_result.context_text}")
        
            # è¡¥å……è¯­ä¹‰ä¿¡æ¯
            if semantic_results:
                context_parts.append("\nã€ç›¸å…³æ–‡æ¡£å†…å®¹ã€‘")
                for i, (chunk, score) in enumerate(semantic_results[:2]):
                    if score > 0.6:  # åªæ·»åŠ é«˜ç›¸å…³æ€§çš„æ–‡æ¡£
                        context_parts.append(f"{i+1}. {chunk.text[:300]}...")  # ä¿®å¤ï¼šä½¿ç”¨ chunk.text
    
        elif query_type == "contextual" and semantic_results:
            # ä¸Šä¸‹æ–‡å‹é—®é¢˜ï¼šä¼˜å…ˆå±•ç¤ºæ–‡æ¡£å†…å®¹
            fusion_method = "semantic_first"
            context_parts.append("ã€ç›¸å…³æ–‡æ¡£å†…å®¹ã€‘")
            for i, (chunk, score) in enumerate(semantic_results):
                context_parts.append(f"{i+1}. (ç›¸å…³æ€§: {score:.3f}) {chunk.text}")  # ä¿®å¤ï¼šä½¿ç”¨ chunk.text
        
            # è¡¥å……å›¾è°±ä¿¡æ¯
            if graph_results:
                for graph_result in graph_results:
                    if graph_result.context_text and "æœªæ‰¾åˆ°" not in graph_result.context_text:
                        context_parts.append(f"\nã€ç›¸å…³çŸ¥è¯†ç‚¹ã€‘\n{graph_result.context_text}")
    
        else:
            # å¹³è¡¡å‹èåˆï¼šäº¤é”™å±•ç¤º
            fusion_method = "balanced"
        
            # å…ˆå±•ç¤ºæœ€ç›¸å…³çš„å›¾è°±ä¿¡æ¯
            if graph_results and graph_results[0].relevance_score > 0.5:
                best_graph = graph_results[0]
                if best_graph.context_text and "æœªæ‰¾åˆ°" not in best_graph.context_text:
                    context_parts.append(f"ã€æ ¸å¿ƒçŸ¥è¯†ã€‘\n{best_graph.context_text}")
        
            # å†å±•ç¤ºè¯­ä¹‰æ£€ç´¢ç»“æœ
            if semantic_results:
                context_parts.append("\nã€ç›¸å…³æ–‡æ¡£ã€‘")
                for i, (chunk, score) in enumerate(semantic_results[:3]):
                    if score > 0.4:  # è¿‡æ»¤ä½ç›¸å…³æ€§ç»“æœ
                        source_info = f"æ¥æº: {chunk.source_file}" if hasattr(chunk, 'source_file') and chunk.source_file else ""
                        # ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ chunk.textï¼ŒDocumentChunk ç±»ä½¿ç”¨çš„æ˜¯ text å­—æ®µ
                        content = chunk.text
                        context_parts.append(f"{i+1}. {content} {source_info}")
        
            # æœ€åè¡¥å……å…¶ä»–å›¾è°±ä¿¡æ¯
            if len(graph_results) > 1:
                for graph_result in graph_results[1:]:
                    if graph_result.context_text and "æœªæ‰¾åˆ°" not in graph_result.context_text:
                        context_parts.append(f"\nã€è¡¥å……ä¿¡æ¯ã€‘\n{graph_result.context_text}")
    
        # å¤„ç†ç©ºç»“æœæƒ…å†µ
        if not context_parts:
            if semantic_results:
                context_parts = [f"æ‰¾åˆ° {len(semantic_results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œä½†ç›¸å…³æ€§è¾ƒä½"]
            elif graph_results:
                context_parts = ["çŸ¥è¯†å›¾è°±ä¸­æ‰¾åˆ°ç›¸å…³æ¦‚å¿µï¼Œä½†ä¿¡æ¯æœ‰é™"]
            else:
                context_parts = ["æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„ä¿¡æ¯"]
            fusion_method = "fallback"
    
        combined_context = "\n\n".join(context_parts)
    
        # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        if len(combined_context) > 2000:
            combined_context = combined_context[:2000] + "\n...(å†…å®¹å·²æˆªæ–­)"
            fusion_method += "_truncated"
    
        return combined_context, fusion_method
    
    def retrieve(self, query: str, top_k: int = 5) -> HybridSearchResult:
        """
        æ‰§è¡Œæ··åˆæ£€ç´¢ - ä¸»æ–¹æ³•
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ··åˆæ£€ç´¢ç»“æœ
        """
        start_time = time.time()
        logger.info(f"ğŸš€ æ··åˆæ£€ç´¢æŸ¥è¯¢: {query}")
        
        # 1. æŸ¥è¯¢ç±»å‹åˆ†ç±»
        query_type = self.classify_query_type(query)
        logger.info(f"æŸ¥è¯¢ç±»å‹: {query_type}")
        
        # 2. å¹¶è¡Œæ‰§è¡Œè¯­ä¹‰å’Œå›¾è°±æ£€ç´¢
        semantic_start = time.time()
        semantic_results = self.semantic_retrieve(query, self.max_semantic_results)
        semantic_time = time.time() - semantic_start
        
        graph_start = time.time()
        graph_results = self.graph_retrieve(query, self.max_graph_results)
        graph_time = time.time() - graph_start
        
        logger.info(f"è¯­ä¹‰æ£€ç´¢: {len(semantic_results)} ä¸ªç»“æœ ({semantic_time:.3f}s)")
        logger.info(f"å›¾è°±æ£€ç´¢: {len(graph_results)} ä¸ªç»“æœ ({graph_time:.3f}s)")
        
        # 3. è®¡ç®—ç»¼åˆå¾—åˆ†
        final_score = self.calculate_combined_score(semantic_results, graph_results, query_type)
        
        # 4. èåˆä¸Šä¸‹æ–‡
        combined_context, fusion_method = self.fuse_contexts(semantic_results, graph_results, query_type)
        
        # 5. ç¡®å®šæœç´¢ç­–ç•¥
        if semantic_results and graph_results:
            search_strategy = f"hybrid_{query_type}"
        elif semantic_results:
            search_strategy = f"semantic_only_{query_type}"
        elif graph_results:
            search_strategy = f"graph_only_{query_type}"
        else:
            search_strategy = "no_results"
        
        total_time = time.time() - start_time
        
        # 6. æ„å»ºç»“æœ
        result = HybridSearchResult(
            semantic_results=semantic_results[:top_k],
            graph_results=graph_results[:top_k],
            combined_context=combined_context,
            final_score=final_score,
            search_strategy=search_strategy,
            total_retrieval_time=total_time,
            fusion_method=fusion_method
        )
        
        logger.info(f"âœ… æ··åˆæ£€ç´¢å®Œæˆ - æ€»è€—æ—¶: {total_time:.3f}s, æœ€ç»ˆå¾—åˆ†: {final_score:.3f}, èåˆæ–¹æ³•: {fusion_method}")
        
        return result
    
    def get_retrieval_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ£€ç´¢å™¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            æ£€ç´¢å™¨ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "semantic_retriever": {
                "model": self.embedding_engine.model_name,
                "chunks_loaded": len(self.embedding_engine.chunks) if hasattr(self.embedding_engine, 'chunks') else 0,
                "embeddings_loaded": len(self.embedding_engine.vectors) if hasattr(self.embedding_engine, 'vectors') and self.embedding_engine.vectors is not None else 0
            },
            "graph_retriever": {
                "connected": hasattr(self.graph_retriever.graph_tool, 'driver') and self.graph_retriever.graph_tool.driver is not None
            },
            "weights": {
                "semantic": self.semantic_weight,
                "graph": self.graph_weight
            },
            "limits": {
                "max_semantic_results": self.max_semantic_results,
                "max_graph_results": self.max_graph_results
            }
        }
        return stats
    
    def update_weights(self, semantic_weight: float, graph_weight: float):
        """
        åŠ¨æ€æ›´æ–°æ£€ç´¢æƒé‡
        
        Args:
            semantic_weight: æ–°çš„è¯­ä¹‰æ£€ç´¢æƒé‡
            graph_weight: æ–°çš„å›¾è°±æ£€ç´¢æƒé‡
        """
        # å½’ä¸€åŒ–æƒé‡
        total_weight = semantic_weight + graph_weight
        if total_weight > 0:
            self.semantic_weight = semantic_weight / total_weight
            self.graph_weight = graph_weight / total_weight
            
            # æ›´æ–°é»˜è®¤é…ç½®
            self.query_type_weights["general"] = {
                "semantic": self.semantic_weight,
                "graph": self.graph_weight
            }
            
            logger.info(f"âœ… æ£€ç´¢æƒé‡å·²æ›´æ–° - è¯­ä¹‰: {self.semantic_weight:.3f}, å›¾è°±: {self.graph_weight:.3f}")
        else:
            logger.warning("âš ï¸ æƒé‡æ›´æ–°å¤±è´¥ï¼šæ€»æƒé‡ä¸èƒ½ä¸º0")
    
    def close(self):
        """å…³é—­æ··åˆæ£€ç´¢å™¨"""
        try:
            if hasattr(self.graph_retriever, 'close'):
                self.graph_retriever.close()
            logger.info("âœ… æ··åˆæ£€ç´¢å™¨å·²å…³é—­")
        except Exception as e:
            logger.error(f"âŒ å…³é—­æ··åˆæ£€ç´¢å™¨æ—¶å‡ºé”™: {e}")

# ä¾¿æ·å‡½æ•°
def create_hybrid_retriever(**kwargs) -> HybridRetriever:
    """åˆ›å»ºæ··åˆæ£€ç´¢å™¨å®ä¾‹"""
    return HybridRetriever(**kwargs)

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸš€ æµ‹è¯•æ··åˆæ£€ç´¢å™¨...")
    
    try:
        # 1. åˆ›å»ºæ··åˆæ£€ç´¢å™¨
        print("\n1ï¸âƒ£ åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨...")
        retriever = HybridRetriever(
            embedding_model="all-mpnet-base-v2",
            semantic_weight=0.6,
            graph_weight=0.4,
            max_semantic_results=5,
            max_graph_results=3
        )
        
        # 2. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\n2ï¸âƒ£ æ£€ç´¢å™¨ç»Ÿè®¡ä¿¡æ¯...")
        stats = retriever.get_retrieval_statistics()
        print(f"ğŸ“Š è¯­ä¹‰æ£€ç´¢:")
        print(f"   æ¨¡å‹: {stats['semantic_retriever']['model']}")
        print(f"   å·²åŠ è½½æ–‡æ¡£å—: {stats['semantic_retriever']['chunks_loaded']}")
        print(f"   å·²åŠ è½½åµŒå…¥å‘é‡: {stats['semantic_retriever']['embeddings_loaded']}")
        print(f"ğŸ“Š å›¾è°±æ£€ç´¢:")
        print(f"   è¿æ¥çŠ¶æ€: {'âœ… å·²è¿æ¥' if stats['graph_retriever']['connected'] else 'âŒ æœªè¿æ¥'}")
        print(f"ğŸ“Š æƒé‡é…ç½®:")
        print(f"   è¯­ä¹‰æ£€ç´¢æƒé‡: {stats['weights']['semantic']:.2f}")
        print(f"   å›¾è°±æ£€ç´¢æƒé‡: {stats['weights']['graph']:.2f}")
        print(f"ğŸ“Š æ£€ç´¢é™åˆ¶:")
        print(f"   æœ€å¤§è¯­ä¹‰ç»“æœæ•°: {stats['limits']['max_semantic_results']}")
        print(f"   æœ€å¤§å›¾è°±ç»“æœæ•°: {stats['limits']['max_graph_results']}")
        
        # 3. æµ‹è¯•æŸ¥è¯¢ç±»å‹åˆ†ç±»
        print("\n3ï¸âƒ£ æµ‹è¯•æŸ¥è¯¢ç±»å‹åˆ†ç±»...")
        test_classification_queries = [
            "ä»€ä¹ˆæ˜¯å¦Šå¨ æœŸç³–å°¿ç—…ï¼Ÿ",           # knowledge_based
            "å¦Šå¨ æœŸç³–å°¿ç—…æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ",       # factual
            "æˆ‘æœ‰ä¸€ä¸ªç³–å°¿ç—…æ‚£è€…çš„ç—…ä¾‹",       # contextual
            "è¡€ç³–å€¼å¤šå°‘ç®—æ­£å¸¸ï¼Ÿ"              # general
        ]
        
        for query in test_classification_queries:
            query_type = retriever.classify_query_type(query)
            print(f"   '{query}' â†’ {query_type}")
        
        # 4. æ‰§è¡Œæ··åˆæ£€ç´¢æµ‹è¯•
        print("\n4ï¸âƒ£ æ‰§è¡Œæ··åˆæ£€ç´¢æµ‹è¯•...")
        test_queries = [
            "å¦Šå¨ æœŸç³–å°¿ç—…çš„ä¸»è¦ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ",      # factual - åº”åé‡å›¾è°±
            "å¦‚ä½•è¯Šæ–­å¦Šå¨ æœŸç³–å°¿ç—…ï¼Ÿ",            # knowledge_based - åº”åé‡å›¾è°±
            "å­•å¦‡è¡€ç³–æ§åˆ¶ä¸å¥½æœ‰ä»€ä¹ˆé£é™©ï¼Ÿ",      # factual - åº”åé‡å›¾è°±
            "ç³–è€é‡æ£€æŸ¥çš„å…·ä½“æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",      # knowledge_based - åº”åé‡å›¾è°±
            "å¦Šå¨ æœŸç³–å°¿ç—…çš„é¥®é£Ÿç®¡ç†è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"  # factual - åº”åé‡å›¾è°±
        ]
        
        total_time = 0
        successful_queries = 0
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢ {i}/{len(test_queries)}: {query}")
            
            try:
                result = retriever.retrieve(query)
                total_time += result.total_retrieval_time
                successful_queries += 1
                
                print(f"   âœ… æŸ¥è¯¢ç±»å‹: {retriever.classify_query_type(query)}")
                print(f"   âœ… æœç´¢ç­–ç•¥: {result.search_strategy}")
                print(f"   âœ… èåˆæ–¹æ³•: {result.fusion_method}")
                print(f"   âœ… æœ€ç»ˆå¾—åˆ†: {result.final_score:.3f}")
                print(f"   âœ… æ£€ç´¢è€—æ—¶: {result.total_retrieval_time:.3f}s")
                print(f"   âœ… è¯­ä¹‰ç»“æœ: {len(result.semantic_results)} ä¸ª")
                print(f"   âœ… å›¾è°±ç»“æœ: {len(result.graph_results)} ä¸ª")
                print(f"   âœ… ä¸Šä¸‹æ–‡é•¿åº¦: {len(result.combined_context)} å­—ç¬¦")
                
                # æ˜¾ç¤ºè¯­ä¹‰ç»“æœè¯¦æƒ…
                if result.semantic_results:
                    print(f"   ğŸ“„ è¯­ä¹‰ç»“æœè¯¦æƒ…:")
                    for j, (chunk, score) in enumerate(result.semantic_results[:2], 1):
                        print(f"      {j}. ç›¸ä¼¼åº¦: {score:.3f} | æ¥æº: {getattr(chunk, 'source_file', 'unknown')}")
                        content_preview = getattr(chunk, 'text', '')[:100]  # DocumentChunk ä½¿ç”¨ text å­—æ®µ
                        print(f"         å†…å®¹é¢„è§ˆ: {content_preview}...")
                
                # æ˜¾ç¤ºå›¾è°±ç»“æœè¯¦æƒ…
                if result.graph_results:
                    print(f"   ğŸ•¸ï¸  å›¾è°±ç»“æœè¯¦æƒ…:")
                    for j, graph_result in enumerate(result.graph_results[:2], 1):
                        print(f"      {j}. ç›¸å…³æ€§: {graph_result.relevance_score:.3f} | å®ä½“æ•°: {len(graph_result.entities)}")
                        if graph_result.entities:
                            entity_names = [entity.name for entity in graph_result.entities[:3]]
                            print(f"         å®ä½“: {', '.join(entity_names)}")
                
                # æ˜¾ç¤ºä¸Šä¸‹æ–‡é¢„è§ˆ
                if result.combined_context:
                    preview = result.combined_context[:300] + "..." if len(result.combined_context) > 300 else result.combined_context
                    print(f"   ğŸ“ ä¸Šä¸‹æ–‡é¢„è§ˆ:\n{preview}")
                
            except Exception as e:
                print(f"   âŒ æŸ¥è¯¢å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # 5. æ€§èƒ½ç»Ÿè®¡
        print(f"\n5ï¸âƒ£ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æ€»æŸ¥è¯¢æ•°: {len(test_queries)}")
        print(f"   æˆåŠŸæŸ¥è¯¢æ•°: {successful_queries}")
        print(f"   æ€»æ£€ç´¢æ—¶é—´: {total_time:.3f}s")
        if successful_queries > 0:
            avg_time = total_time / successful_queries
            print(f"   å¹³å‡æ£€ç´¢æ—¶é—´: {avg_time:.3f}s/æŸ¥è¯¢")
            if avg_time < 1.0:
                print(f"   æ€§èƒ½è¯„çº§: {'ä¼˜ç§€' if avg_time < 0.5 else 'è‰¯å¥½'}")
            else:
                print(f"   æ€§èƒ½è¯„çº§: éœ€è¦ä¼˜åŒ–")
        
        # 6. æµ‹è¯•æƒé‡è°ƒæ•´åŠŸèƒ½
        print(f"\n6ï¸âƒ£ æµ‹è¯•æƒé‡è°ƒæ•´åŠŸèƒ½...")
        print(f"   åŸæƒé‡ - è¯­ä¹‰: {retriever.semantic_weight:.3f}, å›¾è°±: {retriever.graph_weight:.3f}")
        
        # è°ƒæ•´ä¸ºæ›´åé‡å›¾è°±æ£€ç´¢
        retriever.update_weights(0.3, 0.7)
        print(f"   æ–°æƒé‡ - è¯­ä¹‰: {retriever.semantic_weight:.3f}, å›¾è°±: {retriever.graph_weight:.3f}")
        
        # ç”¨è°ƒæ•´åçš„æƒé‡æµ‹è¯•ä¸€ä¸ªæŸ¥è¯¢
        test_query = "å¦Šå¨ æœŸç³–å°¿ç—…æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ"
        print(f"   æµ‹è¯•æŸ¥è¯¢: {test_query}")
        
        adjusted_result = retriever.retrieve(test_query)
        print(f"   è°ƒæ•´æƒé‡åç»“æœ:")
        print(f"     æœ€ç»ˆå¾—åˆ†: {adjusted_result.final_score:.3f}")
        print(f"     æœç´¢ç­–ç•¥: {adjusted_result.search_strategy}")
        print(f"     èåˆæ–¹æ³•: {adjusted_result.fusion_method}")
        
        # 7. æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        print(f"\n7ï¸âƒ£ æµ‹è¯•è¾¹ç•Œæƒ…å†µ...")
        edge_cases = [
            "",                                    # ç©ºæŸ¥è¯¢
            "è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ä¸ç›¸å…³çš„æŸ¥è¯¢å…³äºå¤–æ˜Ÿäºº",    # æ— å…³æŸ¥è¯¢
            "GDM",                                # ç®€çŸ­ç¼©å†™
            "å¦Šå¨ æœŸç³–å°¿ç—…" * 50                    # è¿‡é•¿æŸ¥è¯¢
        ]
        
        for case in edge_cases:
            try:
                if len(case) > 50:
                    display_case = case[:50] + "...(è¿‡é•¿æŸ¥è¯¢)"
                else:
                    display_case = case if case else "(ç©ºæŸ¥è¯¢)"
                
                print(f"   æµ‹è¯•: {display_case}")
                result = retriever.retrieve(case)
                print(f"     ç»“æœ: å¾—åˆ†={result.final_score:.3f}, ç­–ç•¥={result.search_strategy}")
            except Exception as e:
                print(f"     é”™è¯¯: {e}")
        
        # 8. å…³é—­æ£€ç´¢å™¨
        print(f"\n8ï¸âƒ£ å…³é—­æ£€ç´¢å™¨...")
        retriever.close()
        
        print(f"\nâœ… æ··åˆæ£€ç´¢å™¨æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ‰ æ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°è‡´å‘½é”™è¯¯: {e}")
        print(f"ğŸ”§ é”™è¯¯è¯¦æƒ…:")
        import traceback
        traceback.print_exc()
        
        # å°è¯•å…³é—­èµ„æº
        try:
            if 'retriever' in locals():
                retriever.close()
                print("âœ… èµ„æºå·²æ¸…ç†")
        except:
            pass
