"""
GraphRAGæç¤ºè¯æ¨¡æ¿ç³»ç»Ÿï¼ˆGDMï¼‰
æ•´åˆè¯­ä¹‰æ£€ç´¢å’Œå›¾è°±æ£€ç´¢çš„æç¤ºè¯ç”Ÿæˆ
"""

from enum import Enum
from typing import Dict, Optional, Any, List, Tuple
import logging
import re
from dataclasses import dataclass

logger = logging.getLogger(__name__)

class QueryType(Enum):
    """æŸ¥è¯¢ç±»å‹æšä¸¾ - ä¸hybrid_retrieverä¿æŒä¸€è‡´"""
    KNOWLEDGE_BASED = "knowledge_based"
    FACTUAL = "factual"
    CONTEXTUAL = "contextual" 
    GENERAL = "general"
    DIAGNOSTIC = "diagnostic"
    TREATMENT = "treatment"
    RISK_ASSESSMENT = "risk_assessment"

@dataclass
class PromptContext:
    """æç¤ºè¯ä¸Šä¸‹æ–‡æ•°æ®"""
    query: str
    semantic_context: str = ""
    graph_context: str = ""
    query_type: str = "general"
    fusion_method: str = "balanced"
    search_strategy: str = "hybrid"

class MedicalPromptTemplates:
    """
    åŒ»å­¦GraphRAGæç¤ºè¯æ¨¡æ¿ç³»ç»Ÿ
    ä¸ºå¦Šå¨ æœŸç³–å°¿ç—…çŸ¥è¯†é—®ç­”ä¼˜åŒ–ï¼Œä¸æ··åˆæ£€ç´¢å™¨æ·±åº¦é›†æˆ
    """
    
    # ================================
    # ç³»ç»Ÿçº§æç¤ºè¯é…ç½®
    # ================================
    
    SYSTEM_PERSONA = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¦Šå¨ æœŸç³–å°¿ç—…(GDM)åŒ»å­¦åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹è´¨ï¼š
âœ“ æ‹¥æœ‰ä¸°å¯Œçš„å¦Šå¨ æœŸç³–å°¿ç—…ä¸´åºŠçŸ¥è¯†å’Œç ”ç©¶èƒŒæ™¯
âœ“ èƒ½å¤Ÿå‡†ç¡®è§£è¯»åŒ»å­¦æ–‡çŒ®å’Œä¸´åºŠæŒ‡å—
âœ“ å–„äºå°†ä¸“ä¸šåŒ»å­¦çŸ¥è¯†è½¬åŒ–ä¸ºæ‚£è€…æ˜“æ‡‚çš„è¡¨è¾¾
âœ“ ä¸¥æ ¼éµå¾ªåŒ»å­¦ä¼¦ç†å’Œå®‰å…¨å‡†åˆ™

ã€æ ¸å¿ƒåŸåˆ™ã€‘
â€¢ ä¸¥æ ¼åŸºäºæä¾›çš„åŒ»å­¦èµ„æ–™å›ç­”é—®é¢˜
â€¢ ä¿æŒç§‘å­¦ä¸¥è°¨æ€§ï¼Œä¸æä¾›æœªç»è¯å®çš„ä¿¡æ¯
â€¢ æ˜ç¡®åŒºåˆ†ä¸€èˆ¬å¥åº·ä¿¡æ¯å’Œä¸ªæ€§åŒ–åŒ»ç–—å»ºè®®
â€¢ åœ¨é€‚å½“æ—¶æœºå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»å¸ˆ"""

    SAFETY_DISCLAIMER = """
âš ï¸ **é‡è¦åŒ»å­¦å£°æ˜**
æœ¬å›ç­”åŸºäºåŒ»å­¦èµ„æ–™æä¾›å‚è€ƒä¿¡æ¯ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—ã€‚
â€¢ å…·ä½“ç—‡çŠ¶å’Œæ²»ç–—æ–¹æ¡ˆè¯·å’¨è¯¢åŒ»å¸ˆ
â€¢ ç´§æ€¥æƒ…å†µè¯·ç«‹å³å°±åŒ»
â€¢ ç”¨è¯éœ€åœ¨åŒ»å¸ˆæŒ‡å¯¼ä¸‹è¿›è¡Œ"""

    # ================================
    # åŸºç¡€æ¨¡æ¿ç³»åˆ— 
    # ================================

    @staticmethod
    def get_base_template() -> str:
        """åŸºç¡€é—®ç­”æ¨¡æ¿ - é€‚ç”¨äºä¸€èˆ¬æŸ¥è¯¢"""
        return """
{system_persona}

ã€æ£€ç´¢åˆ°çš„åŒ»å­¦çŸ¥è¯†ã€‘
{context}

ã€æ‚£è€…å’¨è¯¢ã€‘
{query}

ã€ä¸“ä¸šå›ç­”è¦æ±‚ã€‘
1. åŸºäºä¸Šè¿°åŒ»å­¦çŸ¥è¯†è¿›è¡Œå›ç­”ï¼Œç¡®ä¿å‡†ç¡®æ€§
2. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„ç»„ç»‡ä¿¡æ¯ï¼ˆå¿…è¦æ—¶ä½¿ç”¨æ ‡é¢˜å’Œè¦ç‚¹ï¼‰
3. å¯¹ä¸“ä¸šæœ¯è¯­è¿›è¡Œé€‚å½“è§£é‡Š
4. å¦‚æœçŸ¥è¯†ä¸è¶³ä»¥å®Œå…¨å›ç­”ï¼Œè¯·æ˜ç¡®è¯´æ˜
5. æä¾›å®ç”¨çš„å»ºè®®å’Œæ³¨æ„äº‹é¡¹

{safety_disclaimer}

ã€ä¸“ä¸šå›ç­”ã€‘
"""

    @staticmethod  
    def get_graph_enhanced_template() -> str:
        """å›¾è°±å¢å¼ºæ¨¡æ¿ - æ•´åˆç»“æ„åŒ–çŸ¥è¯†"""
        return """
{system_persona}

ã€çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‘
{graph_context}

ã€æ–‡çŒ®èµ„æ–™è¡¥å……ã€‘
{semantic_context}

ã€æ‚£è€…å’¨è¯¢ã€‘
{query}

ã€å›¾è°±å¢å¼ºå›ç­”æŒ‡å¼•ã€‘
1. **ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–ä¿¡æ¯**å›ç­”æ ¸å¿ƒé—®é¢˜
2. **ç»“åˆæ–‡çŒ®èµ„æ–™**æä¾›è¯¦ç»†è§£é‡Šå’Œå®ä¾‹æ”¯æ’‘
3. **ä¿æŒä¿¡æ¯ä¸€è‡´æ€§**ï¼Œå¦‚é‡å†²çªè¯·è¯´æ˜å¹¶å»ºè®®å’¨è¯¢åŒ»å¸ˆ
4. **å……åˆ†åˆ©ç”¨å›¾è°±å…³è”**ï¼Œæä¾›ç›¸å…³ä½†ç”¨æˆ·å¯èƒ½æœªæƒ³åˆ°çš„é‡è¦ä¿¡æ¯

å›ç­”ç»“æ„å»ºè®®ï¼š
â€¢ æ ¸å¿ƒå›ç­”ï¼ˆåŸºäºå›¾è°±ï¼‰
â€¢ è¯¦ç»†è¯´æ˜ï¼ˆç»“åˆæ–‡çŒ®ï¼‰
â€¢ ç›¸å…³æé†’ï¼ˆå›¾è°±å…³è”ä¿¡æ¯ï¼‰
â€¢ è¡ŒåŠ¨å»ºè®®

{safety_disclaimer}

ã€ä¸“ä¸šå›ç­”ã€‘
"""

    @staticmethod
    def get_symptom_analysis_template() -> str:
        """ç—‡çŠ¶åˆ†ææ¨¡æ¿ - ä¸“ç”¨äºç—‡çŠ¶ç›¸å…³æŸ¥è¯¢"""
        return """
{system_persona}

ã€ç—‡çŠ¶ç›¸å…³åŒ»å­¦çŸ¥è¯†ã€‘
{context}

ã€ç—‡çŠ¶æè¿°ã€‘
{query}

ã€ç—‡çŠ¶åˆ†ææ¡†æ¶ã€‘
è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¿›è¡Œç—‡çŠ¶åˆ†æï¼š

**1. ç—‡çŠ¶ç‰¹å¾è¯†åˆ«**
- æè¿°ç›¸å…³ç—‡çŠ¶çš„å…¸å‹è¡¨ç°
- è¯´æ˜ç—‡çŠ¶çš„ä¸¥é‡ç¨‹åº¦åˆ†çº§

**2. åŒ»å­¦æœºåˆ¶è§£é‡Š**  
- è§£é‡Šç—‡çŠ¶äº§ç”Ÿçš„ç”Ÿç†ç—…ç†æœºåˆ¶
- ä¸å¦Šå¨ æœŸç³–å°¿ç—…çš„å…³è”æ€§

**3. é‰´åˆ«è¦ç‚¹**
- éœ€è¦ä¸å“ªäº›æƒ…å†µè¿›è¡Œé‰´åˆ«
- ä¼´éšç—‡çŠ¶çš„é‡è¦æ€§

**4. å°±åŒ»æŒ‡å¯¼**
- ä»€ä¹ˆæƒ…å†µä¸‹éœ€è¦å°±åŒ»
- ç´§æ€¥å°±åŒ»çš„å±é™©ä¿¡å·

{safety_disclaimer}

ã€ç—‡çŠ¶åˆ†æã€‘
"""

    @staticmethod
    def get_treatment_guidance_template() -> str:
        """æ²»ç–—æŒ‡å¯¼æ¨¡æ¿ - ä¸“ç”¨äºæ²»ç–—ç›¸å…³æŸ¥è¯¢"""
        return """
{system_persona}

ã€æ²»ç–—ç›¸å…³åŒ»å­¦çŸ¥è¯†ã€‘
{context}

ã€æ²»ç–—å’¨è¯¢ã€‘
{query}

ã€æ²»ç–—æŒ‡å¯¼æ¡†æ¶ã€‘
è¯·æŒ‰ç…§å¾ªè¯åŒ»å­¦åŸåˆ™æä¾›æ²»ç–—æŒ‡å¯¼ï¼š

**1. æ²»ç–—åŸåˆ™å’Œç›®æ ‡**
- åŸºæœ¬æ²»ç–—åŸåˆ™
- é¢„æœŸæ²»ç–—æ•ˆæœå’Œç›®æ ‡

**2. æ²»ç–—æ–¹æ¡ˆå±‚æ¬¡**
- ä¸€çº¿æ²»ç–—æ–¹æ¡ˆï¼ˆç”Ÿæ´»æ–¹å¼å¹²é¢„ï¼‰
- äºŒçº¿æ²»ç–—æ–¹æ¡ˆï¼ˆè¯ç‰©æ²»ç–—ï¼‰
- ç‰¹æ®Šæƒ…å†µå¤„ç†

**3. ç›‘æµ‹å’Œéšè®¿**
- æ²»ç–—æ•ˆæœç›‘æµ‹æŒ‡æ ‡
- éšè®¿æ—¶é—´å’Œé¢‘ç‡
- å‰¯ä½œç”¨ç›‘æµ‹

**4. æ‚£è€…æ•™è‚²è¦ç‚¹**
- æ²»ç–—ä¾ä»æ€§çš„é‡è¦æ€§
- è‡ªæˆ‘ç®¡ç†æŠ€å·§
- å¤æŸ¥æé†’

âš•ï¸ **æ²»ç–—æé†’**ï¼šæ‰€æœ‰æ²»ç–—æ–¹æ¡ˆå¿…é¡»åœ¨åŒ»å¸ˆæŒ‡å¯¼ä¸‹å®æ–½ï¼Œè¯·å‹¿è‡ªè¡Œè°ƒæ•´æ²»ç–—è®¡åˆ’ã€‚

ã€æ²»ç–—æŒ‡å¯¼ã€‘
"""

    @staticmethod
    def get_diagnostic_template() -> str:
        """è¯Šæ–­ç›¸å…³æ¨¡æ¿"""
        return """
{system_persona}

ã€è¯Šæ–­ç›¸å…³åŒ»å­¦çŸ¥è¯†ã€‘
{context}

ã€è¯Šæ–­å’¨è¯¢ã€‘
{query}

ã€è¯Šæ–­ä¿¡æ¯æŒ‡å¯¼ã€‘
åŸºäºå½“å‰åŒ»å­¦æ ‡å‡†ï¼Œæä¾›è¯Šæ–­ç›¸å…³ä¿¡æ¯ï¼š

**1. è¯Šæ–­æ ‡å‡†**
- å›½é™…å…¬è®¤çš„è¯Šæ–­æ ‡å‡†
- è¯Šæ–­çš„å…³é”®æŒ‡æ ‡å’Œæ•°å€¼

**2. æ£€æŸ¥æµç¨‹**
- æ¨èçš„æ£€æŸ¥é¡¹ç›®å’Œé¡ºåº
- æ£€æŸ¥å‰å‡†å¤‡äº‹é¡¹

**3. ç»“æœè§£è¯»**
- æ­£å¸¸å€¼èŒƒå›´
- å¼‚å¸¸ç»“æœçš„ä¸´åºŠæ„ä¹‰

**4. æ³¨æ„äº‹é¡¹**
- å½±å“æ£€æŸ¥å‡†ç¡®æ€§çš„å› ç´ 
- å‡é˜³æ€§/å‡é˜´æ€§çš„å¯èƒ½æ€§

ğŸ“‹ **è¯Šæ–­æé†’**ï¼šè¯Šæ–­ç»“æœé¡»ç”±ä¸“ä¸šåŒ»å¸ˆè§£è¯»ï¼Œè¯·å‹¿è‡ªè¡Œåˆ¤æ–­ã€‚

ã€è¯Šæ–­ä¿¡æ¯ã€‘
"""

    @staticmethod
    def get_risk_assessment_template() -> str:
        """é£é™©è¯„ä¼°æ¨¡æ¿"""
        return """
{system_persona}

ã€é£é™©è¯„ä¼°åŒ»å­¦çŸ¥è¯†ã€‘
{context}

ã€é£é™©å’¨è¯¢ã€‘
{query}

ã€é£é™©è¯„ä¼°æŒ‡å¯¼ã€‘
åŸºäºå¾ªè¯åŒ»å­¦è¯æ®è¿›è¡Œé£é™©è¯„ä¼°ï¼š

**1. é£é™©å› ç´ è¯†åˆ«**
- ä¸»è¦é£é™©å› ç´ åŠå…¶å½±å“ç¨‹åº¦
- å¯æ§åˆ¶vsä¸å¯æ§åˆ¶é£é™©å› ç´ 

**2. é£é™©ç¨‹åº¦è¯„ä¼°**
- ä¸ªä½“é£é™©è¯„ä¼°æ–¹æ³•
- é£é™©åˆ†çº§æ ‡å‡†

**3. é¢„é˜²å’Œæ§åˆ¶ç­–ç•¥**
- ä¸€çº§é¢„é˜²æªæ–½ï¼ˆé¢„é˜²å‘ç”Ÿï¼‰
- äºŒçº§é¢„é˜²æªæ–½ï¼ˆæ—©æœŸå‘ç°ï¼‰
- ä¸‰çº§é¢„é˜²æªæ–½ï¼ˆæ§åˆ¶è¿›å±•ï¼‰

**4. ç›‘æµ‹å»ºè®®**
- é£é™©ç›‘æµ‹æŒ‡æ ‡
- ç›‘æµ‹é¢‘ç‡å’Œæ–¹æ³•

âš ï¸ **é£é™©æé†’**ï¼šä¸ªä½“åŒ–é£é™©è¯„ä¼°éœ€è¦ä¸“ä¸šåŒ»å¸ˆç»¼åˆåˆ¤æ–­ã€‚

ã€é£é™©è¯„ä¼°ã€‘
"""

    @staticmethod
    def get_nutrition_template() -> str:
        """è¥å…»æŒ‡å¯¼æ¨¡æ¿"""
        return """
{system_persona}

ã€è¥å…»ç›¸å…³åŒ»å­¦çŸ¥è¯†ã€‘
{context}

ã€è¥å…»å’¨è¯¢ã€‘
{query}

ã€è¥å…»æŒ‡å¯¼åŸåˆ™ã€‘
åŸºäºå¦Šå¨ æœŸç³–å°¿ç—…è¥å…»ç®¡ç†æŒ‡å—ï¼š

**1. è¥å…»æ²»ç–—ç›®æ ‡**
- è¡€ç³–æ§åˆ¶ç›®æ ‡
- ä½“é‡ç®¡ç†ç›®æ ‡
- è¥å…»éœ€æ±‚æ»¡è¶³

**2. é¥®é£Ÿç»“æ„å»ºè®®**
- ç¢³æ°´åŒ–åˆç‰©é…æ¯”å’Œé€‰æ‹©
- è›‹ç™½è´¨å’Œè„‚è‚ªæ­é…
- å¾®é‡å…ƒç´ è¡¥å……

**3. é¤æ¬¡å®‰æ’**
- ä¸‰é¤ä¸¤ç‚¹åˆ¶åŸåˆ™
- è¿›é¤æ—¶é—´æ§åˆ¶
- ä»½é‡æ§åˆ¶æ–¹æ³•

**4. è¡€ç³–ç›‘æµ‹é…åˆ**
- é¤å‰é¤åè¡€ç³–ç›®æ ‡
- é¥®é£Ÿè°ƒæ•´ä¾æ®

ğŸ **è¥å…»æé†’**ï¼šè¥å…»æ–¹æ¡ˆéœ€è¦ä¸ªä½“åŒ–è°ƒæ•´ï¼Œå»ºè®®å’¨è¯¢è¥å…»å¸ˆåˆ¶å®šè¯¦ç»†è®¡åˆ’ã€‚

ã€è¥å…»æŒ‡å¯¼ã€‘
"""

    @staticmethod
    def get_conversational_template() -> str:
        """å¯¹è¯æ¨¡å¼æ¨¡æ¿ - è€ƒè™‘å†å²å¯¹è¯ä¸Šä¸‹æ–‡"""
        return """
{system_persona}

ã€å¯¹è¯å†å²å›é¡¾ã€‘
{chat_history}

ã€å½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³çŸ¥è¯†ã€‘
{context}

ã€å½“å‰é—®é¢˜ã€‘
{query}

ã€è¿ç»­å¯¹è¯æŒ‡å¯¼ã€‘
ä½œä¸ºä¸“ä¸šåŒ»å­¦åŠ©æ‰‹ï¼Œåœ¨å›ç­”å½“å‰é—®é¢˜æ—¶ï¼š

1. **ä¿æŒå¯¹è¯è¿è´¯æ€§** - å‚è€ƒä¹‹å‰çš„è®¨è®ºå†…å®¹
2. **é¿å…é‡å¤ä¿¡æ¯** - é‡ç‚¹å›ç­”æ–°çš„é—®é¢˜ç‚¹
3. **å»ºç«‹çŸ¥è¯†å…³è”** - å°†æ–°ä¿¡æ¯ä¸ä¹‹å‰è®¨è®ºè”ç³»
4. **ä¸ªæ€§åŒ–å›åº”** - æ ¹æ®ç”¨æˆ·å…³æ³¨ç‚¹è°ƒæ•´å›ç­”é‡ç‚¹

å¦‚æœå½“å‰é—®é¢˜ä¸ä¹‹å‰è®¨è®ºç›¸å…³ï¼Œè¯·é€‚å½“æåŠï¼š"æ ¹æ®æ‚¨ä¹‹å‰çš„é—®é¢˜..."
å¦‚æœæ˜¯æ–°è¯é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜ï¼š"è¿™æ˜¯ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨è¯¦ç»†è§£ç­”..."

{safety_disclaimer}

ã€ä¸“ä¸šå›ç­”ã€‘
"""

class PromptManager:
    """
    æç¤ºè¯ç®¡ç†å™¨ - ä¸HybridRetrieveræ·±åº¦é›†æˆ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æç¤ºè¯ç®¡ç†å™¨"""
        self.templates = MedicalPromptTemplates()
        
        # æ¨¡æ¿æ˜ å°„å…³ç³» - ä¸æ··åˆæ£€ç´¢å™¨çš„æŸ¥è¯¢ç±»å‹ä¿æŒä¸€è‡´
        self.template_mapping = {
            QueryType.KNOWLEDGE_BASED: self.templates.get_base_template,
            QueryType.FACTUAL: self.templates.get_base_template,
            QueryType.CONTEXTUAL: self.templates.get_symptom_analysis_template,
            QueryType.GENERAL: self.templates.get_base_template,
            QueryType.DIAGNOSTIC: self.templates.get_diagnostic_template,
            QueryType.TREATMENT: self.templates.get_treatment_guidance_template,
            QueryType.RISK_ASSESSMENT: self.templates.get_risk_assessment_template
        }
        
        # ç‰¹æ®Šå…³é”®è¯åˆ°æ¨¡æ¿çš„æ˜ å°„
        self.keyword_template_mapping = {
            "ç—‡çŠ¶": self.templates.get_symptom_analysis_template,
            "æ²»ç–—": self.templates.get_treatment_guidance_template,
            "è¯Šæ–­": self.templates.get_diagnostic_template,
            "æ£€æŸ¥": self.templates.get_diagnostic_template,
            "é£é™©": self.templates.get_risk_assessment_template,
            "é¥®é£Ÿ": self.templates.get_nutrition_template,
            "è¥å…»": self.templates.get_nutrition_template
        }
    
    def select_optimal_template(self, prompt_context: PromptContext) -> str:
        """
        æ™ºèƒ½é€‰æ‹©æœ€ä¼˜æ¨¡æ¿ - åŸºäºæŸ¥è¯¢å†…å®¹å’Œæ£€ç´¢ç»“æœ
        
        Args:
            prompt_context: æç¤ºè¯ä¸Šä¸‹æ–‡
            
        Returns:
            é€‰æ‹©çš„æ¨¡æ¿å­—ç¬¦ä¸²
        """
        query = prompt_context.query
        fusion_method = prompt_context.fusion_method
        
        # 1. ä¼˜å…ˆä½¿ç”¨å›¾è°±å¢å¼ºæ¨¡æ¿ï¼ˆå¦‚æœæœ‰å›¾è°±ç»“æœï¼‰
        if fusion_method in ["graph_first", "balanced"] and prompt_context.graph_context:
            return self.templates.get_graph_enhanced_template()
        
        # 2. åŸºäºå…³é”®è¯é€‰æ‹©ä¸“ç”¨æ¨¡æ¿
        for keyword, template_func in self.keyword_template_mapping.items():
            if keyword in query:
                return template_func()
        
        # 3. åŸºäºæŸ¥è¯¢ç±»å‹é€‰æ‹©
        try:
            query_type = QueryType(prompt_context.query_type)
            template_func = self.template_mapping.get(query_type, self.templates.get_base_template)
            return template_func()
        except ValueError:
            return self.templates.get_base_template()
    
    def create_hybrid_prompt(self, 
                           query: str,
                           semantic_results: List[Any] = None,
                           graph_results: List[Any] = None,
                           query_type: str = "general",
                           fusion_method: str = "balanced",
                           chat_history: Optional[str] = None) -> str:
        """
        åˆ›å»ºæ··åˆæç¤ºè¯ - ä¸HybridRetrieverå®Œç¾é›†æˆ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœ List[Tuple[DocumentChunk, float]]
            graph_results: å›¾è°±æ£€ç´¢ç»“æœ List[GraphSearchResult] 
            query_type: æŸ¥è¯¢ç±»å‹
            fusion_method: èåˆæ–¹æ³•
            chat_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºè¯
        """
        # æ„å»ºä¸Šä¸‹æ–‡
        semantic_context = self._build_semantic_context(semantic_results)
        graph_context = self._build_graph_context(graph_results)
        combined_context = self._combine_contexts(semantic_context, graph_context, fusion_method)
        
        # åˆ›å»ºæç¤ºè¯ä¸Šä¸‹æ–‡
        prompt_context = PromptContext(
            query=query,
            semantic_context=semantic_context,
            graph_context=graph_context,
            query_type=query_type,
            fusion_method=fusion_method
        )
        
        # é€‰æ‹©æœ€ä¼˜æ¨¡æ¿
        if chat_history:
            template = self.templates.get_conversational_template()
        else:
            template = self.select_optimal_template(prompt_context)
        
        # å¡«å……æ¨¡æ¿
        try:
            formatted_prompt = template.format(
                system_persona=self.templates.SYSTEM_PERSONA,
                safety_disclaimer=self.templates.SAFETY_DISCLAIMER,
                context=combined_context,
                semantic_context=semantic_context,
                graph_context=graph_context,
                query=query,
                chat_history=chat_history or ""
            )
            
            return self._optimize_prompt_length(formatted_prompt)
            
        except Exception as e:
            logger.error(f"æç¤ºè¯æ ¼å¼åŒ–å¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„åŸºç¡€æç¤ºè¯
            return self._create_fallback_prompt(query, combined_context)
    
    def _build_semantic_context(self, semantic_results: Optional[List[Any]]) -> str:
        """æ„å»ºè¯­ä¹‰æ£€ç´¢ä¸Šä¸‹æ–‡"""
        if not semantic_results:
            return "æš‚æ— ç›¸å…³æ–‡æ¡£èµ„æ–™"
        
        context_parts = []
        for i, (chunk, score) in enumerate(semantic_results[:3], 1):
            source_info = getattr(chunk, 'source_file', 'æœªçŸ¥æ¥æº')
            content = getattr(chunk, 'text', getattr(chunk, 'content', ''))
            
            # é™åˆ¶å•ä¸ªæ–‡æ¡£çš„é•¿åº¦
            content_preview = content[:400] + "..." if len(content) > 400 else content
            
            context_parts.append(
                f"ã€æ–‡æ¡£{i}ã€‘(ç›¸ä¼¼åº¦: {score:.3f}, æ¥æº: {source_info})\n{content_preview}"
            )
        
        return "\n\n".join(context_parts)
    
    def _build_graph_context(self, graph_results: Optional[List[Any]]) -> str:
        """æ„å»ºå›¾è°±æ£€ç´¢ä¸Šä¸‹æ–‡"""
        if not graph_results:
            return "æš‚æ— ç›¸å…³çŸ¥è¯†å›¾è°±ä¿¡æ¯"
        
        context_parts = []
        for i, result in enumerate(graph_results[:2], 1):
            if hasattr(result, 'context_text') and result.context_text:
                context_parts.append(f"ã€çŸ¥è¯†å›¾è°±{i}ã€‘\n{result.context_text}")
        
        return "\n\n".join(context_parts) if context_parts else "çŸ¥è¯†å›¾è°±ä¿¡æ¯æœ‰é™"
    
    def _combine_contexts(self, semantic_context: str, graph_context: str, fusion_method: str) -> str:
        """æ ¹æ®èåˆæ–¹æ³•åˆå¹¶ä¸Šä¸‹æ–‡"""
        if fusion_method == "graph_first":
            return f"{graph_context}\n\n{semantic_context}"
        elif fusion_method == "semantic_first":
            return f"{semantic_context}\n\n{graph_context}"
        else:  # balanced or other
            return f"{graph_context}\n\n{semantic_context}"
    
    def _optimize_prompt_length(self, prompt: str) -> str:
        """ä¼˜åŒ–æç¤ºè¯é•¿åº¦"""
        if len(prompt) > 4000:
            # æˆªæ–­è¿‡é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä¿ç•™é‡è¦éƒ¨åˆ†
            lines = prompt.split('\n')
            important_lines = []
            context_lines = []
            
            for line in lines:
                if any(marker in line for marker in ['ã€', '**', '###', 'ä¸“ä¸šå›ç­”', 'ç³»ç»Ÿæç¤º']):
                    important_lines.append(line)
                else:
                    context_lines.append(line)
            
            # ä¿ç•™é‡è¦è¡Œ + æˆªæ–­çš„ä¸Šä¸‹æ–‡
            truncated_context = '\n'.join(context_lines[:50])  # é™åˆ¶ä¸Šä¸‹æ–‡è¡Œæ•°
            optimized_prompt = '\n'.join(important_lines[:20]) + '\n' + truncated_context
            
            if len(optimized_prompt) > 4000:
                return optimized_prompt[:4000] + "\n...(å†…å®¹å·²ä¼˜åŒ–æˆªæ–­)\n\nã€ä¸“ä¸šå›ç­”ã€‘"
        
        return prompt
    
    def _create_fallback_prompt(self, query: str, context: str) -> str:
        """åˆ›å»ºå¤‡ç”¨æç¤ºè¯"""
        return f"""
{self.templates.SYSTEM_PERSONA}

ã€å¯ç”¨åŒ»å­¦çŸ¥è¯†ã€‘
{context[:1000]}

ã€æ‚£è€…é—®é¢˜ã€‘
{query}

ã€ä¸“ä¸šå›ç­”ã€‘è¯·åŸºäºæä¾›çš„åŒ»å­¦çŸ¥è¯†å›ç­”é—®é¢˜ï¼Œä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ã€‚

{self.templates.SAFETY_DISCLAIMER}
"""

# ================================
# ä¾¿æ·å‡½æ•°å’Œå·¥å‚æ–¹æ³•
# ================================

def create_prompt_manager() -> PromptManager:
    """åˆ›å»ºæç¤ºè¯ç®¡ç†å™¨å®ä¾‹"""
    return PromptManager()

def create_medical_prompt(query: str, 
                         context: str,
                         query_type: str = "general") -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºåŸºç¡€åŒ»å­¦æç¤ºè¯
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        context: æ£€ç´¢ä¸Šä¸‹æ–‡
        query_type: æŸ¥è¯¢ç±»å‹
        
    Returns:
        æ ¼å¼åŒ–çš„æç¤ºè¯
    """
    manager = PromptManager()
    
    # æ¨¡æ‹Ÿè¯­ä¹‰æ£€ç´¢ç»“æœæ ¼å¼
    class MockChunk:
        def __init__(self, text: str):
            self.text = text
            self.content = text
            self.source_file = "åŒ»å­¦èµ„æ–™"
    
    mock_semantic_results = [(MockChunk(context), 0.8)]
    
    return manager.create_hybrid_prompt(
        query=query,
        semantic_results=mock_semantic_results,
        query_type=query_type
    )

def create_graph_enhanced_prompt(query: str,
                               graph_context: str,
                               semantic_context: str = "") -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºå›¾è°±å¢å¼ºæç¤ºè¯
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢  
        graph_context: å›¾è°±ä¸Šä¸‹æ–‡
        semantic_context: è¯­ä¹‰ä¸Šä¸‹æ–‡
        
    Returns:
        å›¾è°±å¢å¼ºçš„æç¤ºè¯
    """
    manager = PromptManager()
    
    # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
    class MockGraphResult:
        def __init__(self, context_text: str):
            self.context_text = context_text
            self.relevance_score = 0.9
    
    class MockChunk:
        def __init__(self, text: str):
            self.text = text
            self.source_file = "åŒ»å­¦æ–‡çŒ®"
    
    mock_graph_results = [MockGraphResult(graph_context)]
    mock_semantic_results = [(MockChunk(semantic_context), 0.7)] if semantic_context else None
    
    return manager.create_hybrid_prompt(
        query=query,
        semantic_results=mock_semantic_results,
        graph_results=mock_graph_results,
        fusion_method="graph_first"
    )

# ================================
# æµ‹è¯•å’ŒéªŒè¯
# ================================

def test_prompt_templates():
    """æµ‹è¯•æç¤ºè¯æ¨¡æ¿ç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•åŒ»å­¦GraphRAGæç¤ºè¯æ¨¡æ¿ç³»ç»Ÿ...")
    
    try:
        manager = PromptManager()
        
        # æµ‹è¯•æ•°æ®
        test_cases = [
            {
                "name": "ç—‡çŠ¶å’¨è¯¢",
                "query": "å¦Šå¨ æœŸç³–å°¿ç—…æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ",
                "query_type": "factual",
                "semantic_context": "å¦Šå¨ æœŸç³–å°¿ç—…çš„å…¸å‹ç—‡çŠ¶åŒ…æ‹¬å¤šå°¿ã€å¤šé¥®ã€ç–²åŠ³ç­‰...",
                "graph_context": "å®ä½“ï¼šå¦Šå¨ æœŸç³–å°¿ç—…\nç—‡çŠ¶ï¼šå¤šå°¿, å¤šé¥®, ä½“é‡å¢åŠ \nå…³è”ï¼šèƒ°å²›ç´ æŠµæŠ—"
            },
            {
                "name": "æ²»ç–—æŒ‡å¯¼", 
                "query": "å¦‚ä½•æ²»ç–—å¦Šå¨ æœŸç³–å°¿ç—…ï¼Ÿ",
                "query_type": "treatment",
                "semantic_context": "å¦Šå¨ æœŸç³–å°¿ç—…çš„æ²»ç–—ä¸»è¦åŒ…æ‹¬é¥®é£Ÿæ§åˆ¶ã€è¿åŠ¨ç–—æ³•å’Œèƒ°å²›ç´ æ²»ç–—...",
                "graph_context": "å®ä½“ï¼šèƒ°å²›ç´ æ²»ç–—\né€‚åº”ç—‡ï¼šå¦Šå¨ æœŸç³–å°¿ç—…\næ•ˆæœï¼šè¡€ç³–æ§åˆ¶"
            },
            {
                "name": "é£é™©è¯„ä¼°",
                "query": "å­•å¦‡è¡€ç³–é«˜æœ‰ä»€ä¹ˆé£é™©ï¼Ÿ", 
                "query_type": "risk_assessment",
                "semantic_context": "å¦Šå¨ æœŸè¡€ç³–å¼‚å¸¸å¯èƒ½å¯¼è‡´æ¯ä½“å’Œèƒå„¿å¹¶å‘ç—‡...",
                "graph_context": "é£é™©å› ç´ ï¼šè¡€ç³–å¼‚å¸¸\nå¹¶å‘ç—‡ï¼šå·¨å¤§å„¿, æ—©äº§, éš¾äº§\nå½±å“å¯¹è±¡ï¼šæ¯äº², èƒå„¿"
            }
        ]
        
        for case in test_cases:
            print(f"\nğŸ“‹ æµ‹è¯•åœºæ™¯ï¼š{case['name']}")
            print(f"   æŸ¥è¯¢ï¼š{case['query']}")
            print(f"   ç±»å‹ï¼š{case['query_type']}")
            
            # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
            class MockChunk:
                def __init__(self, text: str):
                    self.text = text
                    self.source_file = "æµ‹è¯•åŒ»å­¦èµ„æ–™"
            
            class MockGraphResult:
                def __init__(self, context_text: str):
                    self.context_text = context_text
                    self.relevance_score = 0.85
            
            semantic_results = [(MockChunk(case['semantic_context']), 0.8)]
            graph_results = [MockGraphResult(case['graph_context'])]
            
            # ç”Ÿæˆæç¤ºè¯
            prompt = manager.create_hybrid_prompt(
                query=case['query'],
                semantic_results=semantic_results,
                graph_results=graph_results,
                query_type=case['query_type'],
                fusion_method="balanced"
            )
            
            print(f"   âœ… æç¤ºè¯é•¿åº¦ï¼š{len(prompt)} å­—ç¬¦")
            print(f"   ğŸ“ æç¤ºè¯é¢„è§ˆï¼š{prompt[:300]}...")
            
            # ä¿®æ­£åçš„éªŒè¯å…³é”®è¦ç´ é€»è¾‘
            validation_checks = {
                'ç³»ç»Ÿè§’è‰²å®šä¹‰': any(keyword in prompt for keyword in ['åŒ»å­¦åŠ©æ‰‹', 'ä¸“ä¸š', 'ç‰¹è´¨', 'ä¸´åºŠçŸ¥è¯†']),
                'åŒ»å­¦ä¸Šä¸‹æ–‡': any(keyword in prompt for keyword in ['åŒ»å­¦çŸ¥è¯†', 'æ–‡çŒ®èµ„æ–™', 'çŸ¥è¯†å›¾è°±', 'æ£€ç´¢']),
                'ä¸“ä¸šå›ç­”': any(keyword in prompt for keyword in ['ä¸“ä¸šå›ç­”', 'å›ç­”è¦æ±‚', 'åˆ†ææ¡†æ¶', 'æŒ‡å¯¼æ¡†æ¶']),
                'å®‰å…¨å£°æ˜': any(keyword in prompt for keyword in ['åŒ»å­¦å£°æ˜', 'é‡è¦', 'æ›¿ä»£', 'å’¨è¯¢åŒ»å¸ˆ'])
            }
            
            missing_elements = [elem for elem, found in validation_checks.items() if not found]
            
            if missing_elements:
                print(f"   âš ï¸  ç¼ºå°‘è¦ç´ ï¼š{missing_elements}")
            else:
                print(f"   âœ… åŒ…å«æ‰€æœ‰å¿…è¦è¦ç´ ")
        
        # æµ‹è¯•ä¾¿æ·å‡½æ•°
        print(f"\nğŸ”§ æµ‹è¯•ä¾¿æ·å‡½æ•°...")
        
        simple_prompt = create_medical_prompt(
            query="ä»€ä¹ˆæ˜¯å¦Šå¨ æœŸç³–å°¿ç—…ï¼Ÿ",
            context="å¦Šå¨ æœŸç³–å°¿ç—…æ˜¯å­•æœŸå¸¸è§çš„å†…åˆ†æ³Œç–¾ç—…...",
            query_type="knowledge_based"
        )
        print(f"   âœ… åŸºç¡€æç¤ºè¯ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦ï¼š{len(simple_prompt)}")
        
        graph_prompt = create_graph_enhanced_prompt(
            query="å¦Šå¨ æœŸç³–å°¿ç—…å¦‚ä½•è¯Šæ–­ï¼Ÿ",
            graph_context="è¯Šæ–­æ–¹æ³•ï¼šOGTT, ç©ºè…¹è¡€ç³–\næ ‡å‡†ï¼šWHOæ ‡å‡†",
            semantic_context="75gå£æœè‘¡è„ç³–è€é‡è¯•éªŒæ˜¯é‡‘æ ‡å‡†..."
        )
        print(f"   âœ… å›¾è°±å¢å¼ºæç¤ºè¯ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦ï¼š{len(graph_prompt)}")
        
        # æµ‹è¯•å¯¹è¯æ¨¡å¼
        print(f"\nğŸ’¬ æµ‹è¯•å¯¹è¯æ¨¡å¼...")
        chat_history = "ç”¨æˆ·ä¹‹å‰é—®è¿‡å…³äºç—‡çŠ¶çš„é—®é¢˜ï¼Œç°åœ¨æƒ³äº†è§£æ²»ç–—æ–¹æ³•ã€‚"
        
        class MockChunk:
            def __init__(self, text: str):
                self.text = text
                self.source_file = "æµ‹è¯•åŒ»å­¦èµ„æ–™"
        
        conversational_prompt = manager.create_hybrid_prompt(
            query="é‚£åº”è¯¥æ€ä¹ˆæ²»ç–—å‘¢ï¼Ÿ",
            semantic_results=[(MockChunk("æ²»ç–—æ–¹æ¡ˆåŒ…æ‹¬..."), 0.75)],
            query_type="treatment",
            chat_history=chat_history
        )
        print(f"   âœ… å¯¹è¯æç¤ºè¯ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦ï¼š{len(conversational_prompt)}")
        
        # æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        print(f"\nğŸ›¡ï¸ æµ‹è¯•è¾¹ç•Œæƒ…å†µ...")
        
        # ç©ºç»“æœ
        empty_prompt = manager.create_hybrid_prompt(
            query="è¿™æ˜¯ä¸€ä¸ªæ‰¾ä¸åˆ°ç»“æœçš„æŸ¥è¯¢",
            semantic_results=[],
            graph_results=[],
            query_type="general"
        )
        print(f"   âœ… ç©ºç»“æœå¤„ç†æˆåŠŸï¼Œé•¿åº¦ï¼š{len(empty_prompt)}")
        
        # è¶…é•¿å†…å®¹
        long_context = "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„ä¸Šä¸‹æ–‡å†…å®¹..." * 200
        long_prompt = manager.create_hybrid_prompt(
            query="æµ‹è¯•é•¿å†…å®¹",
            semantic_results=[(MockChunk(long_context), 0.6)],
            query_type="general"
        )
        print(f"   âœ… é•¿å†…å®¹ä¼˜åŒ–æˆåŠŸï¼Œé•¿åº¦ï¼š{len(long_prompt)}")
        
        print(f"\nğŸ‰ æç¤ºè¯æ¨¡æ¿ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
        print(f"âœ… æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print(f"ğŸš€ å·²å‡†å¤‡å¥½ä¸GDMé¡¹ç›®é›†æˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
        return False

# ================================
# é«˜çº§åŠŸèƒ½æ‰©å±•
# ================================

class AdvancedPromptFeatures:
    """é«˜çº§æç¤ºè¯åŠŸèƒ½"""
    
    @staticmethod
    def create_multi_turn_prompt(query: str, 
                               conversation_history: List[Dict[str, str]],
                               current_context: str) -> str:
        """
        å¤šè½®å¯¹è¯æç¤ºè¯ç”Ÿæˆ
        
        Args:
            query: å½“å‰æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            current_context: å½“å‰æ£€ç´¢ä¸Šä¸‹æ–‡
            
        Returns:
            å¤šè½®å¯¹è¯ä¼˜åŒ–çš„æç¤ºè¯
        """
        history_text = ""
        for i, turn in enumerate(conversation_history[-3:], 1):  # åªä¿ç•™æœ€è¿‘3è½®
            role = "æ‚£è€…" if turn["role"] == "user" else "åŒ»ç”Ÿ"
            history_text += f"ã€ç¬¬{i}è½®ã€‘{role}ï¼š{turn['content'][:150]}...\n"
        
        return f"""
{MedicalPromptTemplates.SYSTEM_PERSONA}

ã€æœ€è¿‘å¯¹è¯å†å²ã€‘
{history_text}

ã€å½“å‰ç›¸å…³åŒ»å­¦çŸ¥è¯†ã€‘
{current_context}

ã€å½“å‰é—®é¢˜ã€‘
{query}

ã€è¿ç»­å¯¹è¯æŒ‡å¯¼ã€‘
ä½œä¸ºä¸“ä¸šåŒ»å­¦åŠ©æ‰‹ï¼Œåœ¨å›ç­”æ—¶è¦ï¼š
1. ä¿æŒä¸ä¹‹å‰å¯¹è¯çš„è¿è´¯æ€§å’Œä¸€è‡´æ€§
2. é¿å…é‡å¤å·²ç»è¯´è¿‡çš„åŸºç¡€ä¿¡æ¯
3. æ·±å…¥å›ç­”ç”¨æˆ·çš„è¿›ä¸€æ­¥ç–‘é—®
4. å¦‚æœ‰å¿…è¦ï¼Œå¯ä»¥å¼•ç”¨ä¹‹å‰çš„è®¨è®ºå†…å®¹

{MedicalPromptTemplates.SAFETY_DISCLAIMER}

ã€ä¸“ä¸šå›ç­”ã€‘
"""

    @staticmethod
    def create_personalized_prompt(query: str,
                                 context: str,
                                 user_profile: Optional[Dict[str, Any]] = None) -> str:
        """
        ä¸ªæ€§åŒ–æç¤ºè¯ç”Ÿæˆ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: åŒ»å­¦ä¸Šä¸‹æ–‡
            user_profile: ç”¨æˆ·ç”»åƒ {"age_group": "", "pregnancy_stage": "", "risk_level": ""}
            
        Returns:
            ä¸ªæ€§åŒ–çš„æç¤ºè¯
        """
        personalization = ""
        if user_profile:
            age_group = user_profile.get("age_group", "")
            pregnancy_stage = user_profile.get("pregnancy_stage", "")
            risk_level = user_profile.get("risk_level", "")
            
            if age_group or pregnancy_stage or risk_level:
                personalization = f"""
ã€ä¸ªæ€§åŒ–ä¿¡æ¯å‚è€ƒã€‘
å¹´é¾„é˜¶æ®µï¼š{age_group}
å¦Šå¨ é˜¶æ®µï¼š{pregnancy_stage}  
é£é™©ç­‰çº§ï¼š{risk_level}

è¯·åœ¨å›ç­”æ—¶è€ƒè™‘ä¸Šè¿°ä¸ªäººæƒ…å†µï¼Œæä¾›æ›´æœ‰é’ˆå¯¹æ€§çš„å»ºè®®ã€‚
"""

        return f"""
{MedicalPromptTemplates.SYSTEM_PERSONA}

{personalization}

ã€åŒ»å­¦çŸ¥è¯†èµ„æ–™ã€‘
{context}

ã€ä¸ªäººå’¨è¯¢ã€‘
{query}

ã€ä¸ªæ€§åŒ–å›ç­”è¦æ±‚ã€‘
1. ç»“åˆç”¨æˆ·çš„å…·ä½“æƒ…å†µæä¾›å»ºè®®
2. å¼ºè°ƒä¸ªä½“å·®å¼‚å’Œä¸ªæ€§åŒ–æ²»ç–—çš„é‡è¦æ€§
3. æä¾›åˆ†å±‚æ¬¡çš„å»ºè®®ï¼ˆè½»ã€ä¸­ã€é‡ä¸åŒæƒ…å†µï¼‰
4. å¿…è¦æ—¶å»ºè®®å¯»æ±‚ä¸ªæ€§åŒ–çš„ä¸“ä¸šåŒ»ç–—æ„è§

{MedicalPromptTemplates.SAFETY_DISCLAIMER}

ã€ä¸“ä¸šå›ç­”ã€‘
"""

class PromptQualityValidator:
    """æç¤ºè¯è´¨é‡éªŒè¯å™¨"""
    
    @staticmethod
    def validate_prompt_quality(prompt: str) -> Dict[str, Any]:
        """
        éªŒè¯æç¤ºè¯è´¨é‡
        
        Args:
            prompt: å¾…éªŒè¯çš„æç¤ºè¯
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        result = {
            "is_valid": True,
            "issues": [],
            "suggestions": [],
            "quality_score": 0,
            "metrics": {}
        }
        
        # 1. é•¿åº¦æ£€æŸ¥
        length = len(prompt)
        result["metrics"]["length"] = length
        
        if length < 100:
            result["issues"].append("æç¤ºè¯è¿‡çŸ­ï¼Œå¯èƒ½ç¼ºå°‘å¿…è¦ä¿¡æ¯")
            result["is_valid"] = False
        elif length > 5000:
            result["issues"].append("æç¤ºè¯è¿‡é•¿ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½")
            result["suggestions"].append("è€ƒè™‘ç²¾ç®€ä¸Šä¸‹æ–‡å†…å®¹")
        
        # 2. å¿…éœ€å…ƒç´ æ£€æŸ¥
        required_elements = [
            ("ç³»ç»Ÿæç¤º", ["ç³»ç»Ÿ", "åŠ©æ‰‹", "ä¸“ä¸š", "åŒ»å­¦åŠ©æ‰‹", "è§’è‰²", "èº«ä»½", "ç‰¹è´¨"]),
            ("åŒ»å­¦ä¸Šä¸‹æ–‡", ["åŒ»å­¦", "çŸ¥è¯†", "èµ„æ–™", "æ–‡çŒ®", "å›¾è°±", "æ£€ç´¢", "ä¸Šä¸‹æ–‡", "ä¿¡æ¯"]),
            ("ç”¨æˆ·æŸ¥è¯¢", ["é—®é¢˜", "å’¨è¯¢", "æŸ¥è¯¢", "æ‚£è€…", "ç—‡çŠ¶", "æ²»ç–—"]),
            ("å®‰å…¨å£°æ˜", ["å£°æ˜", "æé†’", "åŒ»å¸ˆ", "å°±åŒ»", "æ›¿ä»£", "å‚è€ƒ", "å»ºè®®", "ä¸“ä¸šåŒ»ç–—"])
        ]
        
        missing_elements = []
        for element_name, keywords in required_elements:
            # æ›´å®½æ¾çš„åŒ¹é…é€»è¾‘
            found = any(keyword in prompt.lower() for keyword in [kw.lower() for kw in keywords])
            if not found:
                missing_elements.append(element_name)
        
        if missing_elements:
            result["issues"].append(f"ç¼ºå°‘å¿…éœ€å…ƒç´ ï¼š{', '.join(missing_elements)}")
            result["is_valid"] = False
        
        # 3. æ ¼å¼è§„èŒƒæ£€æŸ¥
        format_issues = []
        if "ã€" not in prompt or "ã€‘" not in prompt:
            format_issues.append("ç¼ºå°‘æ ‡å‡†çš„ä¸­æ–‡æ ‡é¢˜æ ¼å¼")
        
        if not re.search(r'\*\*.*?\*\*', prompt) and not re.search(r'âœ“|â€¢', prompt):
            format_issues.append("å»ºè®®ä½¿ç”¨ç²—ä½“æ ‡è®°æˆ–ç¬¦å·æ ‡è®°é‡è¦ä¿¡æ¯")
        
        if format_issues:
            result["suggestions"].extend(format_issues)
        
        # 4. åŒ»å­¦ä¸“ä¸šæ€§æ£€æŸ¥
        medical_keywords = ["ç—‡çŠ¶", "è¯Šæ–­", "æ²»ç–—", "è¯ç‰©", "æ£€æŸ¥", "è¡€ç³–", "èƒ°å²›ç´ ", "å¦Šå¨ ", "åŒ»å­¦", "ä¸´åºŠ"]
        medical_score = sum(1 for keyword in medical_keywords if keyword in prompt)
        result["metrics"]["medical_relevance"] = medical_score / len(medical_keywords)
        
        if medical_score < 2:
            result["suggestions"].append("å»ºè®®å¢åŠ æ›´å¤šåŒ»å­¦ç›¸å…³æœ¯è¯­å’Œæ¦‚å¿µ")
        
        # 5. è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        base_score = 60
        
        # é•¿åº¦åˆ†æ•° (0-15åˆ†)
        if 200 <= length <= 3000:
            length_score = 15
        elif 100 <= length < 200 or 3000 < length <= 4000:
            length_score = 10
        else:
            length_score = 5
        
        # å®Œæ•´æ€§åˆ†æ•° (0-15åˆ†)
        completeness_score = 15 if not missing_elements else max(0, 15 - len(missing_elements) * 4)
        
        # ä¸“ä¸šæ€§åˆ†æ•° (0-10åˆ†)
        professional_score = min(10, medical_score * 1.5)
        
        result["quality_score"] = base_score + length_score + completeness_score + professional_score
        
        return result

class PromptOptimizer:
    """æç¤ºè¯ä¼˜åŒ–å™¨"""
    
    @staticmethod
    def optimize_for_token_limit(prompt: str, max_tokens: int = 3500) -> str:
        """
        é’ˆå¯¹tokené™åˆ¶ä¼˜åŒ–æç¤ºè¯
        
        Args:
            prompt: åŸå§‹æç¤ºè¯
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆä¼°ç®—ï¼Œ1 token â‰ˆ 1.2-1.5ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
            
        Returns:
            ä¼˜åŒ–åçš„æç¤ºè¯
        """
        # ä¼°ç®—å½“å‰tokenæ•°
        estimated_tokens = len(prompt) // 1.3
        
        if estimated_tokens <= max_tokens:
            return prompt
        
        logger.info(f"æç¤ºè¯è¶…å‡ºtokené™åˆ¶ï¼Œå¼€å§‹ä¼˜åŒ–ï¼š{estimated_tokens} > {max_tokens}")
        
        # åˆ†è§£æç¤ºè¯ç»“æ„
        sections = re.split(r'ã€.*?ã€‘', prompt)
        headers = re.findall(r'ã€.*?ã€‘', prompt)
        
        # ä¼˜å…ˆçº§æ’åºï¼ˆä¿ç•™é‡è¦éƒ¨åˆ†ï¼‰
        priority_keywords = {
            "ç³»ç»Ÿ": 10, "ä¸“ä¸š": 9, "å›ç­”": 9, "å®‰å…¨": 8,
            "çŸ¥è¯†": 7, "åŒ»å­¦": 7, "é—®é¢˜": 6, "å’¨è¯¢": 6
        }
        
        # æŒ‰ä¼˜å…ˆçº§é‡ç»„
        important_sections = []
        context_sections = []
        
        for i, (header, section) in enumerate(zip(headers, sections[1:], strict=False)):
            priority = max([priority_keywords.get(keyword, 0) 
                          for keyword in priority_keywords.keys() 
                          if keyword in header], default=1)
            
            if priority >= 7:
                important_sections.append((header, section))
            else:
                context_sections.append((header, section))
        
        # é‡å»ºæç¤ºè¯
        optimized_prompt = sections[0]  # å¼€å¤´éƒ¨åˆ†
        
        # æ·»åŠ é‡è¦éƒ¨åˆ†
        for header, section in important_sections:
            optimized_prompt += header + section
        
        # æŒ‰éœ€æ·»åŠ ä¸Šä¸‹æ–‡éƒ¨åˆ†
        remaining_tokens = max_tokens - len(optimized_prompt) // 1.3
        
        for header, section in context_sections:
            section_tokens = len(section) // 1.3
            if section_tokens < remaining_tokens:
                optimized_prompt += header + section
                remaining_tokens -= section_tokens
            else:
                # æˆªæ–­å¤„ç†
                truncated_length = int(remaining_tokens * 1.3 * 0.8)  # ç•™20%ä½™é‡
                truncated_section = section[:truncated_length] + "...(å†…å®¹å·²æˆªæ–­)"
                optimized_prompt += header + truncated_section
                break
        
        logger.info(f"æç¤ºè¯ä¼˜åŒ–å®Œæˆï¼š{len(optimized_prompt)//1.3} tokens")
        return optimized_prompt
    
    @staticmethod
    def enhance_medical_safety(prompt: str) -> str:
        """
        å¢å¼ºåŒ»å­¦å®‰å…¨æ€§
        
        Args:
            prompt: åŸå§‹æç¤ºè¯
            
        Returns:
            å¢å¼ºå®‰å…¨æ€§çš„æç¤ºè¯
        """
        safety_enhancements = [
            "\nâš•ï¸ ã€åŒ»ç–—å®‰å…¨å¼ºåŒ–æé†’ã€‘",
            "â€¢ æœ¬ä¿¡æ¯ä»…ä¾›å¥åº·æ•™è‚²å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£åŒ»ç”Ÿçš„ä¸“ä¸šè¯Šæ–­",
            "â€¢ ä»»ä½•ç—‡çŠ¶å˜åŒ–æˆ–æ²»ç–—è°ƒæ•´éƒ½åº”å’¨è¯¢åŒ»ç–—ä¸“ä¸šäººå‘˜",
            "â€¢ ç´§æ€¥æƒ…å†µï¼ˆå¦‚ä¸¥é‡é«˜è¡€ç³–ã€é…®ç—‡é…¸ä¸­æ¯’å¾è±¡ï¼‰è¯·ç«‹å³å°±åŒ»",
            "â€¢ å¦Šå¨ æœŸé—´çš„ä»»ä½•åŒ»ç–—å†³å®šéƒ½éœ€è¦äº§ç§‘åŒ»ç”Ÿå‚ä¸",
        ]
        
        # åœ¨å›ç­”è¦æ±‚å‰æ’å…¥å®‰å…¨å¢å¼º
        safety_text = "\n".join(safety_enhancements)
        
        # æ‰¾åˆ°æ’å…¥ä½ç½®ï¼ˆé€šå¸¸åœ¨"ä¸“ä¸šå›ç­”"å‰ï¼‰
        insert_patterns = [
            "ã€ä¸“ä¸šå›ç­”ã€‘", "ã€å›ç­”ã€‘", "ã€åŒ»ç”Ÿå›ç­”ã€‘", "ã€ä¸“ä¸šå»ºè®®ã€‘"
        ]
        
        for pattern in insert_patterns:
            if pattern in prompt:
                return prompt.replace(pattern, safety_text + "\n\n" + pattern)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ¨¡å¼ï¼Œåœ¨æœ«å°¾æ·»åŠ 
        return prompt + safety_text + "\n\nã€ä¸“ä¸šå›ç­”ã€‘"

# ================================
# é›†æˆæ¥å£å’Œå¯¼å‡º
# ================================

class GraphRAGPromptInterface:
    """
    GraphRAGæç¤ºè¯æ¥å£ - ä¸ä¸»ç³»ç»Ÿé›†æˆçš„æ ‡å‡†æ¥å£
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æ¥å£
        
        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or {}
        self.prompt_manager = PromptManager()
        self.optimizer = PromptOptimizer()
        self.validator = PromptQualityValidator()
        
        # ä»é…ç½®ä¸­è¯»å–å‚æ•°
        self.max_tokens = self.config.get('max_tokens', 3500)
        self.enable_safety_enhancement = self.config.get('enable_safety_enhancement', True)
        self.enable_optimization = self.config.get('enable_optimization', True)
    
    def create_prompt(self,
                     query: str,
                     semantic_results: Optional[List[Any]] = None,
                     graph_results: Optional[List[Any]] = None,
                     query_type: str = "general",
                     fusion_method: str = "balanced",
                     chat_history: Optional[str] = None,
                     user_profile: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        åˆ›å»ºä¼˜åŒ–çš„æç¤ºè¯ - ä¸»è¦å¯¹å¤–æ¥å£
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœ
            graph_results: å›¾è°±æ£€ç´¢ç»“æœ
            query_type: æŸ¥è¯¢ç±»å‹
            fusion_method: èåˆæ–¹æ³•
            chat_history: å¯¹è¯å†å²
            user_profile: ç”¨æˆ·ç”»åƒ
            
        Returns:
            åŒ…å«æç¤ºè¯å’Œè´¨é‡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # 1. ç”ŸæˆåŸºç¡€æç¤ºè¯
            if user_profile and self.config.get('enable_personalization', False):
                # ä¸ªæ€§åŒ–æç¤ºè¯
                context = self.prompt_manager._build_semantic_context(semantic_results)
                base_prompt = AdvancedPromptFeatures.create_personalized_prompt(
                    query, context, user_profile
                )
            else:
                # æ ‡å‡†æ··åˆæç¤ºè¯
                base_prompt = self.prompt_manager.create_hybrid_prompt(
                    query=query,
                    semantic_results=semantic_results,
                    graph_results=graph_results,
                    query_type=query_type,
                    fusion_method=fusion_method,
                    chat_history=chat_history
                )
            
            # 2. å®‰å…¨æ€§å¢å¼º
            if self.enable_safety_enhancement:
                base_prompt = self.optimizer.enhance_medical_safety(base_prompt)
            
            # 3. é•¿åº¦ä¼˜åŒ–
            if self.enable_optimization:
                optimized_prompt = self.optimizer.optimize_for_token_limit(
                    base_prompt, self.max_tokens
                )
            else:
                optimized_prompt = base_prompt
            
            # 4. è´¨é‡éªŒè¯
            quality_result = self.validator.validate_prompt_quality(optimized_prompt)
            
            return {
                "prompt": optimized_prompt,
                "quality_score": quality_result["quality_score"],
                "is_valid": quality_result["is_valid"],
                "issues": quality_result["issues"],
                "suggestions": quality_result["suggestions"],
                "metrics": {
                    "original_length": len(base_prompt),
                    "optimized_length": len(optimized_prompt),
                    "estimated_tokens": len(optimized_prompt) // 1.3,
                    "medical_relevance": quality_result["metrics"].get("medical_relevance", 0)
                }
            }
            
        except Exception as e:
            logger.error(f"æç¤ºè¯åˆ›å»ºå¤±è´¥ï¼š{str(e)}")
            
            # è¿”å›å®‰å…¨çš„å¤‡ç”¨æç¤ºè¯
            fallback_prompt = self.prompt_manager._create_fallback_prompt(
                query, "åŸºç¡€åŒ»å­¦çŸ¥è¯†åº“"
            )
            
            return {
                "prompt": fallback_prompt,
                "quality_score": 60,
                "is_valid": True,
                "issues": [f"ä½¿ç”¨å¤‡ç”¨æç¤ºè¯ï¼š{str(e)}"],
                "suggestions": ["å»ºè®®æ£€æŸ¥è¾“å…¥å‚æ•°çš„å®Œæ•´æ€§"],
                "metrics": {
                    "original_length": len(fallback_prompt),
                    "optimized_length": len(fallback_prompt),
                    "estimated_tokens": len(fallback_prompt) // 1.3,
                    "medical_relevance": 0.3
                }
            }

# ================================
# æ¨¡å—å¯¼å‡ºå’Œé»˜è®¤é…ç½®
# ================================

# é»˜è®¤é…ç½®
DEFAULT_CONFIG = {
    "max_tokens": 3500,
    "enable_safety_enhancement": True,
    "enable_optimization": True,
    "enable_personalization": False,
    "quality_threshold": 70
}

# ä¸»è¦å¯¼å‡ºæ¥å£
__all__ = [
    'MedicalPromptTemplates',
    'PromptManager', 
    'GraphRAGPromptInterface',
    'QueryType',
    'PromptContext',
    'create_prompt_manager',
    'create_medical_prompt',
    'create_graph_enhanced_prompt',
    'test_prompt_templates',
    'DEFAULT_CONFIG'
]

def create_gdm_prompt_interface(config: Optional[Dict[str, Any]] = None) -> GraphRAGPromptInterface:
    """
    åˆ›å»ºGDMé¡¹ç›®ä¸“ç”¨çš„æç¤ºè¯æ¥å£
    
    Args:
        config: è‡ªå®šä¹‰é…ç½®ï¼Œä¼šä¸é»˜è®¤é…ç½®åˆå¹¶
        
    Returns:
        é…ç½®å¥½çš„æç¤ºè¯æ¥å£å®ä¾‹
    """
    merged_config = DEFAULT_CONFIG.copy()
    if config:
        merged_config.update(config)
    
    return GraphRAGPromptInterface(merged_config)

# æ¨¡å—åˆå§‹åŒ–æ—¶çš„è‡ªæ£€
if __name__ == "__main__":
    print("ğŸš€ GraphRAGåŒ»å­¦æç¤ºè¯æ¨¡æ¿ç³»ç»Ÿ")
    print("=" * 50)
    
    # è¿è¡Œè‡ªæ£€æµ‹è¯•
    test_result = test_prompt_templates()
    
    if test_result:
        print("\nâœ… æ¨¡å—è‡ªæ£€é€šè¿‡ï¼Œå‡†å¤‡é›†æˆåˆ°GDMé¡¹ç›®ï¼")
        print("\nğŸ“‹ æ¨èä½¿ç”¨æ–¹å¼ï¼š")
        print("```python")
        print("from src.graphrag.prompt_templates import create_gdm_prompt_interface")
        print("")
        print("# åˆ›å»ºæç¤ºè¯æ¥å£")
        print("prompt_interface = create_gdm_prompt_interface()")
        print("")
        print("# ç”Ÿæˆæç¤ºè¯") 
        print("result = prompt_interface.create_prompt(")
        print('    query="å¦Šå¨ æœŸç³–å°¿ç—…æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ",')
        print("    semantic_results=semantic_results,")
        print("    graph_results=graph_results,")
        print('    query_type="factual"')
        print(")")
        print("")
        print("print(result['prompt'])")
        print("```")
    else:
        print("\nâŒ æ¨¡å—è‡ªæ£€å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç é—®é¢˜ï¼")
